==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Model                                    [128, 42]                 --
├─Sequential: 1-1                        [128, 64, 64, 64]         --
│    └─Conv2d: 2-1                       [128, 64, 64, 64]         4,864
│    └─BatchNorm2d: 2-2                  [128, 64, 64, 64]         128
│    └─ReLU: 2-3                         [128, 64, 64, 64]         --
├─Sequential: 1-2                        [128, 128, 32, 32]        --
│    └─Conv2d: 2-4                       [128, 128, 64, 64]        73,856
│    └─BatchNorm2d: 2-5                  [128, 128, 64, 64]        256
│    └─ReLU: 2-6                         [128, 128, 64, 64]        --
│    └─MaxPool2d: 2-7                    [128, 128, 32, 32]        --
├─Sequential: 1-3                        [128, 128, 32, 32]        --
│    └─Sequential: 2-8                   [128, 128, 32, 32]        --
│    │    └─Conv2d: 3-1                  [128, 128, 32, 32]        147,584
│    │    └─BatchNorm2d: 3-2             [128, 128, 32, 32]        256
│    │    └─ReLU: 3-3                    [128, 128, 32, 32]        --
│    └─Sequential: 2-9                   [128, 128, 32, 32]        --
│    │    └─Conv2d: 3-4                  [128, 128, 32, 32]        147,584
│    │    └─BatchNorm2d: 3-5             [128, 128, 32, 32]        256
│    │    └─ReLU: 3-6                    [128, 128, 32, 32]        --
├─Sequential: 1-4                        [128, 256, 16, 16]        --
│    └─Conv2d: 2-10                      [128, 256, 32, 32]        295,168
│    └─BatchNorm2d: 2-11                 [128, 256, 32, 32]        512
│    └─ReLU: 2-12                        [128, 256, 32, 32]        --
│    └─MaxPool2d: 2-13                   [128, 256, 16, 16]        --
├─Sequential: 1-5                        [128, 512, 8, 8]          --
│    └─Conv2d: 2-14                      [128, 512, 16, 16]        1,180,160
│    └─BatchNorm2d: 2-15                 [128, 512, 16, 16]        1,024
│    └─ReLU: 2-16                        [128, 512, 16, 16]        --
│    └─MaxPool2d: 2-17                   [128, 512, 8, 8]          --
├─Sequential: 1-6                        [128, 512, 8, 8]          --
│    └─Sequential: 2-18                  [128, 512, 8, 8]          --
│    │    └─Conv2d: 3-7                  [128, 512, 8, 8]          2,359,808
│    │    └─BatchNorm2d: 3-8             [128, 512, 8, 8]          1,024
│    │    └─ReLU: 3-9                    [128, 512, 8, 8]          --
│    └─Sequential: 2-19                  [128, 512, 8, 8]          --
│    │    └─Conv2d: 3-10                 [128, 512, 8, 8]          2,359,808
│    │    └─BatchNorm2d: 3-11            [128, 512, 8, 8]          1,024
│    │    └─ReLU: 3-12                   [128, 512, 8, 8]          --
├─Sequential: 1-7                        [128, 1024, 4, 4]         --
│    └─Conv2d: 2-20                      [128, 1024, 8, 8]         4,719,616
│    └─BatchNorm2d: 2-21                 [128, 1024, 8, 8]         2,048
│    └─ReLU: 2-22                        [128, 1024, 8, 8]         --
│    └─MaxPool2d: 2-23                   [128, 1024, 4, 4]         --
├─Sequential: 1-8                        [128, 42]                 --
│    └─MaxPool2d: 2-24                   [128, 1024, 1, 1]         --
│    └─Flatten: 2-25                     [128, 1024]               --
│    └─Linear: 2-26                      [128, 512]                524,800
│    └─Linear: 2-27                      [128, 128]                65,664
│    └─Linear: 2-28                      [128, 42]                 5,418
==========================================================================================
Total params: 11,890,858
Trainable params: 11,890,858
Non-trainable params: 0
Total mult-adds (G): 234.72
==========================================================================================
Input size (MB): 6.29
Forward/backward pass size (MB): 3221.92
Params size (MB): 47.56
Estimated Total Size (MB): 3275.78
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Model                                    [128, 42]                 --
├─Sequential: 1-1                        [128, 64, 64, 64]         --
│    └─Conv2d: 2-1                       [128, 64, 64, 64]         4,864
│    └─BatchNorm2d: 2-2                  [128, 64, 64, 64]         128
│    └─ReLU: 2-3                         [128, 64, 64, 64]         --
├─Sequential: 1-2                        [128, 128, 32, 32]        --
│    └─Conv2d: 2-4                       [128, 128, 64, 64]        73,856
│    └─BatchNorm2d: 2-5                  [128, 128, 64, 64]        256
│    └─ReLU: 2-6                         [128, 128, 64, 64]        --
│    └─MaxPool2d: 2-7                    [128, 128, 32, 32]        --
├─Sequential: 1-3                        [128, 128, 32, 32]        --
│    └─Sequential: 2-8                   [128, 128, 32, 32]        --
│    │    └─Conv2d: 3-1                  [128, 128, 32, 32]        147,584
│    │    └─BatchNorm2d: 3-2             [128, 128, 32, 32]        256
│    │    └─ReLU: 3-3                    [128, 128, 32, 32]        --
│    └─Sequential: 2-9                   [128, 128, 32, 32]        --
│    │    └─Conv2d: 3-4                  [128, 128, 32, 32]        147,584
│    │    └─BatchNorm2d: 3-5             [128, 128, 32, 32]        256
│    │    └─ReLU: 3-6                    [128, 128, 32, 32]        --
├─Sequential: 1-4                        [128, 256, 16, 16]        --
│    └─Conv2d: 2-10                      [128, 256, 32, 32]        295,168
│    └─BatchNorm2d: 2-11                 [128, 256, 32, 32]        512
│    └─ReLU: 2-12                        [128, 256, 32, 32]        --
│    └─MaxPool2d: 2-13                   [128, 256, 16, 16]        --
├─Sequential: 1-5                        [128, 512, 8, 8]          --
│    └─Conv2d: 2-14                      [128, 512, 16, 16]        1,180,160
│    └─BatchNorm2d: 2-15                 [128, 512, 16, 16]        1,024
│    └─ReLU: 2-16                        [128, 512, 16, 16]        --
│    └─MaxPool2d: 2-17                   [128, 512, 8, 8]          --
├─Sequential: 1-6                        [128, 512, 8, 8]          --
│    └─Sequential: 2-18                  [128, 512, 8, 8]          --
│    │    └─Conv2d: 3-7                  [128, 512, 8, 8]          2,359,808
│    │    └─BatchNorm2d: 3-8             [128, 512, 8, 8]          1,024
│    │    └─ReLU: 3-9                    [128, 512, 8, 8]          --
│    └─Sequential: 2-19                  [128, 512, 8, 8]          --
│    │    └─Conv2d: 3-10                 [128, 512, 8, 8]          2,359,808
│    │    └─BatchNorm2d: 3-11            [128, 512, 8, 8]          1,024
│    │    └─ReLU: 3-12                   [128, 512, 8, 8]          --
├─Sequential: 1-7                        [128, 1024, 4, 4]         --
│    └─Conv2d: 2-20                      [128, 1024, 8, 8]         4,719,616
│    └─BatchNorm2d: 2-21                 [128, 1024, 8, 8]         2,048
│    └─ReLU: 2-22                        [128, 1024, 8, 8]         --
│    └─MaxPool2d: 2-23                   [128, 1024, 4, 4]         --
├─Sequential: 1-8                        [128, 42]                 --
│    └─MaxPool2d: 2-24                   [128, 1024, 1, 1]         --
│    └─Flatten: 2-25                     [128, 1024]               --
│    └─Linear: 2-26                      [128, 512]                524,800
│    └─Linear: 2-27                      [128, 128]                65,664
│    └─Linear: 2-28                      [128, 42]                 5,418
==========================================================================================
Total params: 11,890,858
Trainable params: 11,890,858
Non-trainable params: 0
Total mult-adds (G): 234.72
==========================================================================================
Input size (MB): 6.29
Forward/backward pass size (MB): 3221.92
Params size (MB): 47.56
Estimated Total Size (MB): 3275.78
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Model                                    [128, 42]                 --
├─Sequential: 1-1                        [128, 64, 64, 64]         --
│    └─Conv2d: 2-1                       [128, 64, 64, 64]         4,864
│    └─BatchNorm2d: 2-2                  [128, 64, 64, 64]         128
│    └─ReLU: 2-3                         [128, 64, 64, 64]         --
├─Sequential: 1-2                        [128, 128, 32, 32]        --
│    └─Conv2d: 2-4                       [128, 128, 64, 64]        73,856
│    └─BatchNorm2d: 2-5                  [128, 128, 64, 64]        256
│    └─ReLU: 2-6                         [128, 128, 64, 64]        --
│    └─MaxPool2d: 2-7                    [128, 128, 32, 32]        --
├─Sequential: 1-3                        [128, 128, 32, 32]        --
│    └─Sequential: 2-8                   [128, 128, 32, 32]        --
│    │    └─Conv2d: 3-1                  [128, 128, 32, 32]        147,584
│    │    └─BatchNorm2d: 3-2             [128, 128, 32, 32]        256
│    │    └─ReLU: 3-3                    [128, 128, 32, 32]        --
│    └─Sequential: 2-9                   [128, 128, 32, 32]        --
│    │    └─Conv2d: 3-4                  [128, 128, 32, 32]        147,584
│    │    └─BatchNorm2d: 3-5             [128, 128, 32, 32]        256
│    │    └─ReLU: 3-6                    [128, 128, 32, 32]        --
├─Sequential: 1-4                        [128, 256, 16, 16]        --
│    └─Conv2d: 2-10                      [128, 256, 32, 32]        295,168
│    └─BatchNorm2d: 2-11                 [128, 256, 32, 32]        512
│    └─ReLU: 2-12                        [128, 256, 32, 32]        --
│    └─MaxPool2d: 2-13                   [128, 256, 16, 16]        --
├─Sequential: 1-5                        [128, 512, 8, 8]          --
│    └─Conv2d: 2-14                      [128, 512, 16, 16]        1,180,160
│    └─BatchNorm2d: 2-15                 [128, 512, 16, 16]        1,024
│    └─ReLU: 2-16                        [128, 512, 16, 16]        --
│    └─MaxPool2d: 2-17                   [128, 512, 8, 8]          --
├─Sequential: 1-6                        [128, 512, 8, 8]          --
│    └─Sequential: 2-18                  [128, 512, 8, 8]          --
│    │    └─Conv2d: 3-7                  [128, 512, 8, 8]          2,359,808
│    │    └─BatchNorm2d: 3-8             [128, 512, 8, 8]          1,024
│    │    └─ReLU: 3-9                    [128, 512, 8, 8]          --
│    └─Sequential: 2-19                  [128, 512, 8, 8]          --
│    │    └─Conv2d: 3-10                 [128, 512, 8, 8]          2,359,808
│    │    └─BatchNorm2d: 3-11            [128, 512, 8, 8]          1,024
│    │    └─ReLU: 3-12                   [128, 512, 8, 8]          --
├─Sequential: 1-7                        [128, 1024, 4, 4]         --
│    └─Conv2d: 2-20                      [128, 1024, 8, 8]         4,719,616
│    └─BatchNorm2d: 2-21                 [128, 1024, 8, 8]         2,048
│    └─ReLU: 2-22                        [128, 1024, 8, 8]         --
│    └─MaxPool2d: 2-23                   [128, 1024, 4, 4]         --
├─Sequential: 1-8                        [128, 42]                 --
│    └─MaxPool2d: 2-24                   [128, 1024, 1, 1]         --
│    └─Flatten: 2-25                     [128, 1024]               --
│    └─Linear: 2-26                      [128, 512]                524,800
│    └─Linear: 2-27                      [128, 128]                65,664
│    └─Linear: 2-28                      [128, 42]                 5,418
==========================================================================================
Total params: 11,890,858
Trainable params: 11,890,858
Non-trainable params: 0
Total mult-adds (G): 234.72
==========================================================================================
Input size (MB): 6.29
Forward/backward pass size (MB): 3221.92
Params size (MB): 47.56
Estimated Total Size (MB): 3275.78
==========================================================================================
  0%|          | 0/150 [00:00<?, ?it/s]acc/rec/prec: 0.62/0.31/0.39  | train/valid loss: 1.7392/1.4174:   0%|          | 0/150 [00:30<?, ?it/s]acc/rec/prec: 0.62/0.31/0.39  | train/valid loss: 1.7392/1.4174:   1%|          | 1/150 [00:30<1:16:23, 30.76s/it]acc/rec/prec: 0.80/0.45/0.53  | train/valid loss: 0.6768/0.7360:   1%|          | 1/150 [01:01<1:16:23, 30.76s/it]acc/rec/prec: 0.80/0.45/0.53  | train/valid loss: 0.6768/0.7360:   1%|▏         | 2/150 [01:01<1:15:54, 30.77s/it]acc/rec/prec: 0.82/0.54/0.66  | train/valid loss: 0.4209/0.6764:   1%|▏         | 2/150 [01:32<1:15:54, 30.77s/it]acc/rec/prec: 0.82/0.54/0.66  | train/valid loss: 0.4209/0.6764:   2%|▏         | 3/150 [01:32<1:15:36, 30.86s/it]acc/rec/prec: 0.83/0.56/0.71  | train/valid loss: 0.2681/0.6341:   2%|▏         | 3/150 [02:03<1:15:36, 30.86s/it]acc/rec/prec: 0.83/0.56/0.71  | train/valid loss: 0.2681/0.6341:   3%|▎         | 4/150 [02:03<1:15:13, 30.92s/it]acc/rec/prec: 0.85/0.63/0.71  | train/valid loss: 0.1556/0.6465:   3%|▎         | 4/150 [02:34<1:15:13, 30.92s/it]acc/rec/prec: 0.85/0.63/0.71  | train/valid loss: 0.1556/0.6465:   3%|▎         | 5/150 [02:34<1:15:03, 31.06s/it]Saved weights with acc/rec/prec: 0.85/0.63/0.71 | loss: 0.6465
acc/rec/prec: 0.90/0.68/0.78  | train/valid loss: 0.0850/0.4123:   3%|▎         | 5/150 [03:05<1:15:03, 31.06s/it]acc/rec/prec: 0.90/0.68/0.78  | train/valid loss: 0.0850/0.4123:   4%|▍         | 6/150 [03:05<1:14:33, 31.07s/it]Saved weights with acc/rec/prec: 0.90/0.68/0.78 | loss: 0.4123
acc/rec/prec: 0.92/0.74/0.79  | train/valid loss: 0.0311/0.3235:   4%|▍         | 6/150 [03:37<1:14:33, 31.07s/it]acc/rec/prec: 0.92/0.74/0.79  | train/valid loss: 0.0311/0.3235:   5%|▍         | 7/150 [03:37<1:14:11, 31.13s/it]Saved weights with acc/rec/prec: 0.92/0.74/0.79 | loss: 0.3235
acc/rec/prec: 0.94/0.81/0.81  | train/valid loss: 0.0119/0.2772:   5%|▍         | 7/150 [04:08<1:14:11, 31.13s/it]acc/rec/prec: 0.94/0.81/0.81  | train/valid loss: 0.0119/0.2772:   5%|▌         | 8/150 [04:08<1:13:48, 31.19s/it]Saved weights with acc/rec/prec: 0.94/0.81/0.81 | loss: 0.2772
acc/rec/prec: 0.93/0.82/0.86  | train/valid loss: 0.0082/0.2943:   5%|▌         | 8/150 [04:39<1:13:48, 31.19s/it]acc/rec/prec: 0.93/0.82/0.86  | train/valid loss: 0.0082/0.2943:   6%|▌         | 9/150 [04:39<1:12:58, 31.05s/it]acc/rec/prec: 0.94/0.83/0.84  | train/valid loss: 0.0072/0.2859:   6%|▌         | 9/150 [05:10<1:12:58, 31.05s/it]acc/rec/prec: 0.94/0.83/0.84  | train/valid loss: 0.0072/0.2859:   7%|▋         | 10/150 [05:10<1:12:40, 31.14s/it]acc/rec/prec: 0.94/0.81/0.86  | train/valid loss: 0.0028/0.2772:   7%|▋         | 10/150 [05:41<1:12:40, 31.14s/it]acc/rec/prec: 0.94/0.81/0.86  | train/valid loss: 0.0028/0.2772:   7%|▋         | 11/150 [05:41<1:12:07, 31.13s/it]acc/rec/prec: 0.94/0.84/0.86  | train/valid loss: 0.0028/0.2639:   7%|▋         | 11/150 [06:12<1:12:07, 31.13s/it]acc/rec/prec: 0.94/0.84/0.86  | train/valid loss: 0.0028/0.2639:   8%|▊         | 12/150 [06:12<1:11:34, 31.12s/it]Saved weights with acc/rec/prec: 0.94/0.84/0.86 | loss: 0.2639
acc/rec/prec: 0.94/0.81/0.87  | train/valid loss: 0.0025/0.2623:   8%|▊         | 12/150 [06:44<1:11:34, 31.12s/it]acc/rec/prec: 0.94/0.81/0.87  | train/valid loss: 0.0025/0.2623:   9%|▊         | 13/150 [06:44<1:11:08, 31.16s/it]Saved weights with acc/rec/prec: 0.94/0.81/0.87 | loss: 0.2623
acc/rec/prec: 0.94/0.83/0.85  | train/valid loss: 0.0022/0.2520:   9%|▊         | 13/150 [07:15<1:11:08, 31.16s/it]acc/rec/prec: 0.94/0.83/0.85  | train/valid loss: 0.0022/0.2520:   9%|▉         | 14/150 [07:15<1:10:31, 31.11s/it]Saved weights with acc/rec/prec: 0.94/0.83/0.85 | loss: 0.2520
acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0013/0.2571:   9%|▉         | 14/150 [07:45<1:10:31, 31.11s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0013/0.2571:  10%|█         | 15/150 [07:45<1:09:54, 31.07s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0019/0.2567:  10%|█         | 15/150 [08:16<1:09:54, 31.07s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0019/0.2567:  11%|█         | 16/150 [08:16<1:09:18, 31.03s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0014/0.2565:  11%|█         | 16/150 [08:48<1:09:18, 31.03s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0014/0.2565:  11%|█▏        | 17/150 [08:48<1:08:51, 31.07s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0008/0.2580:  11%|█▏        | 17/150 [09:19<1:08:51, 31.07s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0008/0.2580:  12%|█▏        | 18/150 [09:19<1:08:27, 31.12s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0010/0.2553:  12%|█▏        | 18/150 [09:50<1:08:27, 31.12s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0010/0.2553:  13%|█▎        | 19/150 [09:50<1:07:54, 31.10s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0011/0.2550:  13%|█▎        | 19/150 [10:21<1:07:54, 31.10s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0011/0.2550:  13%|█▎        | 20/150 [10:21<1:07:24, 31.11s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0010/0.2578:  13%|█▎        | 20/150 [10:52<1:07:24, 31.11s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0010/0.2578:  14%|█▍        | 21/150 [10:52<1:07:00, 31.17s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0009/0.2581:  14%|█▍        | 21/150 [11:23<1:07:00, 31.17s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0009/0.2581:  15%|█▍        | 22/150 [11:23<1:06:26, 31.14s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0008/0.2609:  15%|█▍        | 22/150 [11:54<1:06:26, 31.14s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0008/0.2609:  15%|█▌        | 23/150 [11:54<1:05:53, 31.13s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0007/0.2608:  15%|█▌        | 23/150 [12:26<1:05:53, 31.13s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0007/0.2608:  16%|█▌        | 24/150 [12:26<1:05:36, 31.24s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0007/0.2607:  16%|█▌        | 24/150 [12:57<1:05:36, 31.24s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0007/0.2607:  17%|█▋        | 25/150 [12:57<1:05:06, 31.25s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0006/0.2603:  17%|█▋        | 25/150 [13:28<1:05:06, 31.25s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0006/0.2603:  17%|█▋        | 26/150 [13:28<1:04:19, 31.13s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0005/0.2623:  17%|█▋        | 26/150 [13:59<1:04:19, 31.13s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0005/0.2623:  18%|█▊        | 27/150 [13:59<1:03:42, 31.08s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0004/0.2607:  18%|█▊        | 27/150 [14:30<1:03:42, 31.08s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0004/0.2607:  19%|█▊        | 28/150 [14:30<1:03:03, 31.01s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2606:  19%|█▊        | 28/150 [15:01<1:03:03, 31.01s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2606:  19%|█▉        | 29/150 [15:01<1:02:34, 31.03s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2607:  19%|█▉        | 29/150 [15:33<1:02:34, 31.03s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2607:  20%|██        | 30/150 [15:33<1:02:20, 31.17s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0004/0.2587:  20%|██        | 30/150 [16:04<1:02:20, 31.17s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0004/0.2587:  21%|██        | 31/150 [16:04<1:01:43, 31.12s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2594:  21%|██        | 31/150 [16:34<1:01:43, 31.12s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2594:  21%|██▏       | 32/150 [16:34<1:01:05, 31.06s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2630:  21%|██▏       | 32/150 [17:06<1:01:05, 31.06s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2630:  22%|██▏       | 33/150 [17:06<1:00:37, 31.09s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2618:  22%|██▏       | 33/150 [17:37<1:00:37, 31.09s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2618:  23%|██▎       | 34/150 [17:37<1:00:03, 31.07s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2600:  23%|██▎       | 34/150 [18:08<1:00:03, 31.07s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2600:  23%|██▎       | 35/150 [18:08<59:30, 31.05s/it]  acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2602:  23%|██▎       | 35/150 [18:39<59:30, 31.05s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2602:  24%|██▍       | 36/150 [18:39<58:54, 31.01s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2629:  24%|██▍       | 36/150 [19:10<58:54, 31.01s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2629:  25%|██▍       | 37/150 [19:10<58:25, 31.02s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0004/0.2618:  25%|██▍       | 37/150 [19:41<58:25, 31.02s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0004/0.2618:  25%|██▌       | 38/150 [19:41<58:03, 31.11s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0003/0.2575:  25%|██▌       | 38/150 [20:12<58:03, 31.11s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0003/0.2575:  26%|██▌       | 39/150 [20:12<57:34, 31.12s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2599:  26%|██▌       | 39/150 [20:43<57:34, 31.12s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2599:  27%|██▋       | 40/150 [20:43<57:11, 31.20s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2597:  27%|██▋       | 40/150 [21:15<57:11, 31.20s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2597:  27%|██▋       | 41/150 [21:15<56:37, 31.17s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2625:  27%|██▋       | 41/150 [21:46<56:37, 31.17s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2625:  28%|██▊       | 42/150 [21:46<56:07, 31.18s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2620:  28%|██▊       | 42/150 [22:17<56:07, 31.18s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2620:  29%|██▊       | 43/150 [22:17<55:41, 31.23s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2613:  29%|██▊       | 43/150 [22:48<55:41, 31.23s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2613:  29%|██▉       | 44/150 [22:48<55:00, 31.14s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2642:  29%|██▉       | 44/150 [23:19<55:00, 31.14s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2642:  30%|███       | 45/150 [23:19<54:27, 31.12s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2599:  30%|███       | 45/150 [23:50<54:27, 31.12s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2599:  31%|███       | 46/150 [23:50<53:52, 31.09s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2601:  31%|███       | 46/150 [24:21<53:52, 31.09s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2601:  31%|███▏      | 47/150 [24:21<53:21, 31.08s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2652:  31%|███▏      | 47/150 [24:52<53:21, 31.08s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2652:  32%|███▏      | 48/150 [24:52<52:52, 31.11s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2649:  32%|███▏      | 48/150 [25:23<52:52, 31.11s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2649:  33%|███▎      | 49/150 [25:23<52:23, 31.13s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2633:  33%|███▎      | 49/150 [25:54<52:23, 31.13s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2633:  33%|███▎      | 50/150 [25:54<51:45, 31.05s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2617:  33%|███▎      | 50/150 [26:25<51:45, 31.05s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2617:  34%|███▍      | 51/150 [26:25<51:15, 31.06s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2647:  34%|███▍      | 51/150 [26:57<51:15, 31.06s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2647:  35%|███▍      | 52/150 [26:57<50:45, 31.08s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2613:  35%|███▍      | 52/150 [27:27<50:45, 31.08s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2613:  35%|███▌      | 53/150 [27:27<50:08, 31.02s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2622:  35%|███▌      | 53/150 [27:58<50:08, 31.02s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2622:  36%|███▌      | 54/150 [27:58<49:32, 30.97s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2637:  36%|███▌      | 54/150 [28:29<49:32, 30.97s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2637:  37%|███▋      | 55/150 [28:29<49:06, 31.02s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2620:  37%|███▋      | 55/150 [29:00<49:06, 31.02s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2620:  37%|███▋      | 56/150 [29:00<48:30, 30.97s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2608:  37%|███▋      | 56/150 [29:31<48:30, 30.97s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2608:  38%|███▊      | 57/150 [29:31<48:02, 30.99s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2613:  38%|███▊      | 57/150 [30:03<48:02, 30.99s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2613:  39%|███▊      | 58/150 [30:03<47:40, 31.09s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2598:  39%|███▊      | 58/150 [30:34<47:40, 31.09s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2598:  39%|███▉      | 59/150 [30:34<47:06, 31.06s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2625:  39%|███▉      | 59/150 [31:05<47:06, 31.06s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2625:  40%|████      | 60/150 [31:05<46:38, 31.09s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0004/0.2597:  40%|████      | 60/150 [31:36<46:38, 31.09s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0004/0.2597:  41%|████      | 61/150 [31:36<46:05, 31.07s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2601:  41%|████      | 61/150 [32:07<46:05, 31.07s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2601:  41%|████▏     | 62/150 [32:07<45:35, 31.09s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2584:  41%|████▏     | 62/150 [32:38<45:35, 31.09s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2584:  42%|████▏     | 63/150 [32:38<45:02, 31.07s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2613:  42%|████▏     | 63/150 [33:09<45:02, 31.07s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2613:  43%|████▎     | 64/150 [33:09<44:29, 31.04s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2620:  43%|████▎     | 64/150 [33:40<44:29, 31.04s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2620:  43%|████▎     | 65/150 [33:40<43:56, 31.01s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2588:  43%|████▎     | 65/150 [34:11<43:56, 31.01s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2588:  44%|████▍     | 66/150 [34:11<43:26, 31.03s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2642:  44%|████▍     | 66/150 [34:42<43:26, 31.03s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2642:  45%|████▍     | 67/150 [34:42<42:55, 31.03s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2592:  45%|████▍     | 67/150 [35:13<42:55, 31.03s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2592:  45%|████▌     | 68/150 [35:13<42:33, 31.14s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2616:  45%|████▌     | 68/150 [35:44<42:33, 31.14s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2616:  46%|████▌     | 69/150 [35:44<42:01, 31.14s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2605:  46%|████▌     | 69/150 [36:15<42:01, 31.14s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2605:  47%|████▋     | 70/150 [36:15<41:27, 31.09s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2607:  47%|████▋     | 70/150 [36:46<41:27, 31.09s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2607:  47%|████▋     | 71/150 [36:46<40:52, 31.04s/it]acc/rec/prec: 0.95/0.85/0.85  | train/valid loss: 0.0004/0.2636:  47%|████▋     | 71/150 [37:17<40:52, 31.04s/it]acc/rec/prec: 0.95/0.85/0.85  | train/valid loss: 0.0004/0.2636:  48%|████▊     | 72/150 [37:17<40:19, 31.02s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2601:  48%|████▊     | 72/150 [37:49<40:19, 31.02s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2601:  49%|████▊     | 73/150 [37:49<39:58, 31.16s/it]acc/rec/prec: 0.95/0.84/0.85  | train/valid loss: 0.0004/0.2598:  49%|████▊     | 73/150 [38:20<39:58, 31.16s/it]acc/rec/prec: 0.95/0.84/0.85  | train/valid loss: 0.0004/0.2598:  49%|████▉     | 74/150 [38:20<39:23, 31.10s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2611:  49%|████▉     | 74/150 [38:51<39:23, 31.10s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2611:  50%|█████     | 75/150 [38:51<38:52, 31.11s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2626:  50%|█████     | 75/150 [39:22<38:52, 31.11s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2626:  51%|█████     | 76/150 [39:22<38:16, 31.03s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2617:  51%|█████     | 76/150 [39:53<38:16, 31.03s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2617:  51%|█████▏    | 77/150 [39:53<37:49, 31.08s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2621:  51%|█████▏    | 77/150 [40:24<37:49, 31.08s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2621:  52%|█████▏    | 78/150 [40:24<37:12, 31.01s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2618:  52%|█████▏    | 78/150 [40:55<37:12, 31.01s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2618:  53%|█████▎    | 79/150 [40:55<36:49, 31.11s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2595:  53%|█████▎    | 79/150 [41:26<36:49, 31.11s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2595:  53%|█████▎    | 80/150 [41:26<36:20, 31.15s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0004/0.2638:  53%|█████▎    | 80/150 [41:58<36:20, 31.15s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0004/0.2638:  54%|█████▍    | 81/150 [41:58<35:56, 31.26s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0004/0.2610:  54%|█████▍    | 81/150 [42:29<35:56, 31.26s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0004/0.2610:  55%|█████▍    | 82/150 [42:29<35:15, 31.12s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2611:  55%|█████▍    | 82/150 [42:59<35:15, 31.12s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2611:  55%|█████▌    | 83/150 [42:59<34:36, 30.99s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2611:  55%|█████▌    | 83/150 [43:30<34:36, 30.99s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2611:  56%|█████▌    | 84/150 [43:30<33:59, 30.91s/it]acc/rec/prec: 0.95/0.84/0.85  | train/valid loss: 0.0004/0.2612:  56%|█████▌    | 84/150 [44:01<33:59, 30.91s/it]acc/rec/prec: 0.95/0.84/0.85  | train/valid loss: 0.0004/0.2612:  57%|█████▋    | 85/150 [44:01<33:33, 30.98s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2603:  57%|█████▋    | 85/150 [44:33<33:33, 30.98s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2603:  57%|█████▋    | 86/150 [44:33<33:08, 31.06s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0004/0.2597:  57%|█████▋    | 86/150 [45:04<33:08, 31.06s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0004/0.2597:  58%|█████▊    | 87/150 [45:04<32:39, 31.10s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2607:  58%|█████▊    | 87/150 [45:35<32:39, 31.10s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2607:  59%|█████▊    | 88/150 [45:35<32:06, 31.07s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2596:  59%|█████▊    | 88/150 [46:06<32:06, 31.07s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2596:  59%|█████▉    | 89/150 [46:06<31:39, 31.13s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0004/0.2611:  59%|█████▉    | 89/150 [46:38<31:39, 31.13s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0004/0.2611:  60%|██████    | 90/150 [46:38<31:14, 31.24s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2618:  60%|██████    | 90/150 [47:09<31:14, 31.24s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2618:  61%|██████    | 91/150 [47:09<30:44, 31.26s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2605:  61%|██████    | 91/150 [47:40<30:44, 31.26s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2605:  61%|██████▏   | 92/150 [47:40<30:13, 31.26s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0004/0.2605:  61%|██████▏   | 92/150 [48:11<30:13, 31.26s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0004/0.2605:  62%|██████▏   | 93/150 [48:11<29:41, 31.26s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2609:  62%|██████▏   | 93/150 [48:42<29:41, 31.26s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2609:  63%|██████▎   | 94/150 [48:42<29:04, 31.16s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2618:  63%|██████▎   | 94/150 [49:13<29:04, 31.16s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2618:  63%|██████▎   | 95/150 [49:13<28:28, 31.06s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2618:  63%|██████▎   | 95/150 [49:44<28:28, 31.06s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2618:  64%|██████▍   | 96/150 [49:44<28:00, 31.12s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2597:  64%|██████▍   | 96/150 [50:16<28:00, 31.12s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2597:  65%|██████▍   | 97/150 [50:16<27:35, 31.24s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2599:  65%|██████▍   | 97/150 [50:47<27:35, 31.24s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2599:  65%|██████▌   | 98/150 [50:47<27:01, 31.19s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2618:  65%|██████▌   | 98/150 [51:18<27:01, 31.19s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2618:  66%|██████▌   | 99/150 [51:18<26:33, 31.25s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2604:  66%|██████▌   | 99/150 [51:50<26:33, 31.25s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2604:  67%|██████▋   | 100/150 [51:50<26:03, 31.27s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2600:  67%|██████▋   | 100/150 [52:21<26:03, 31.27s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2600:  67%|██████▋   | 101/150 [52:21<25:30, 31.24s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2606:  67%|██████▋   | 101/150 [52:52<25:30, 31.24s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2606:  68%|██████▊   | 102/150 [52:52<25:01, 31.28s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2624:  68%|██████▊   | 102/150 [53:23<25:01, 31.28s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2624:  69%|██████▊   | 103/150 [53:23<24:29, 31.26s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2590:  69%|██████▊   | 103/150 [53:55<24:29, 31.26s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2590:  69%|██████▉   | 104/150 [53:55<23:57, 31.26s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2605:  69%|██████▉   | 104/150 [54:26<23:57, 31.26s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2605:  70%|███████   | 105/150 [54:26<23:26, 31.25s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2602:  70%|███████   | 105/150 [54:57<23:26, 31.25s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2602:  71%|███████   | 106/150 [54:57<22:55, 31.27s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2595:  71%|███████   | 106/150 [55:29<22:55, 31.27s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2595:  71%|███████▏  | 107/150 [55:29<22:25, 31.30s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2606:  71%|███████▏  | 107/150 [56:00<22:25, 31.30s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2606:  72%|███████▏  | 108/150 [56:00<21:54, 31.30s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2617:  72%|███████▏  | 108/150 [56:31<21:54, 31.30s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2617:  73%|███████▎  | 109/150 [56:31<21:21, 31.27s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2617:  73%|███████▎  | 109/150 [57:02<21:21, 31.27s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2617:  73%|███████▎  | 110/150 [57:02<20:51, 31.28s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2614:  73%|███████▎  | 110/150 [57:34<20:51, 31.28s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2614:  74%|███████▍  | 111/150 [57:34<20:18, 31.24s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0004/0.2589:  74%|███████▍  | 111/150 [58:04<20:18, 31.24s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0004/0.2589:  75%|███████▍  | 112/150 [58:04<19:43, 31.15s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2593:  75%|███████▍  | 112/150 [58:36<19:43, 31.15s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2593:  75%|███████▌  | 113/150 [58:36<19:15, 31.22s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2598:  75%|███████▌  | 113/150 [59:07<19:15, 31.22s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2598:  76%|███████▌  | 114/150 [59:07<18:43, 31.20s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2600:  76%|███████▌  | 114/150 [59:38<18:43, 31.20s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2600:  77%|███████▋  | 115/150 [59:38<18:08, 31.11s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2591:  77%|███████▋  | 115/150 [1:00:09<18:08, 31.11s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2591:  77%|███████▋  | 116/150 [1:00:09<17:37, 31.10s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2590:  77%|███████▋  | 116/150 [1:00:40<17:37, 31.10s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2590:  78%|███████▊  | 117/150 [1:00:40<17:07, 31.12s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2590:  78%|███████▊  | 117/150 [1:01:11<17:07, 31.12s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2590:  79%|███████▊  | 118/150 [1:01:11<16:37, 31.16s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2612:  79%|███████▊  | 118/150 [1:01:43<16:37, 31.16s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2612:  79%|███████▉  | 119/150 [1:01:43<16:09, 31.27s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2626:  79%|███████▉  | 119/150 [1:02:14<16:09, 31.27s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2626:  80%|████████  | 120/150 [1:02:14<15:38, 31.29s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2617:  80%|████████  | 120/150 [1:02:46<15:38, 31.29s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2617:  81%|████████  | 121/150 [1:02:46<15:08, 31.33s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2597:  81%|████████  | 121/150 [1:03:17<15:08, 31.33s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2597:  81%|████████▏ | 122/150 [1:03:17<14:36, 31.30s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2591:  81%|████████▏ | 122/150 [1:03:48<14:36, 31.30s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2591:  82%|████████▏ | 123/150 [1:03:48<14:03, 31.24s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2620:  82%|████████▏ | 123/150 [1:04:19<14:03, 31.24s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2620:  83%|████████▎ | 124/150 [1:04:19<13:31, 31.21s/it]acc/rec/prec: 0.95/0.84/0.85  | train/valid loss: 0.0004/0.2591:  83%|████████▎ | 124/150 [1:04:51<13:31, 31.21s/it]acc/rec/prec: 0.95/0.84/0.85  | train/valid loss: 0.0004/0.2591:  83%|████████▎ | 125/150 [1:04:51<13:01, 31.26s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2614:  83%|████████▎ | 125/150 [1:05:22<13:01, 31.26s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2614:  84%|████████▍ | 126/150 [1:05:22<12:30, 31.26s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2631:  84%|████████▍ | 126/150 [1:05:53<12:30, 31.26s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2631:  85%|████████▍ | 127/150 [1:05:53<11:59, 31.26s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0004/0.2602:  85%|████████▍ | 127/150 [1:06:24<11:59, 31.26s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0004/0.2602:  85%|████████▌ | 128/150 [1:06:24<11:27, 31.26s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0004/0.2606:  85%|████████▌ | 128/150 [1:06:55<11:27, 31.26s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0004/0.2606:  86%|████████▌ | 129/150 [1:06:55<10:55, 31.22s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2619:  86%|████████▌ | 129/150 [1:07:27<10:55, 31.22s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2619:  87%|████████▋ | 130/150 [1:07:27<10:24, 31.21s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2610:  87%|████████▋ | 130/150 [1:07:58<10:24, 31.21s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2610:  87%|████████▋ | 131/150 [1:07:58<09:51, 31.15s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2611:  87%|████████▋ | 131/150 [1:08:29<09:51, 31.15s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2611:  88%|████████▊ | 132/150 [1:08:29<09:20, 31.15s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2608:  88%|████████▊ | 132/150 [1:09:00<09:20, 31.15s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2608:  89%|████████▊ | 133/150 [1:09:00<08:48, 31.11s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2588:  89%|████████▊ | 133/150 [1:09:31<08:48, 31.11s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2588:  89%|████████▉ | 134/150 [1:09:31<08:18, 31.17s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2619:  89%|████████▉ | 134/150 [1:10:02<08:18, 31.17s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2619:  90%|█████████ | 135/150 [1:10:02<07:46, 31.13s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2620:  90%|█████████ | 135/150 [1:10:33<07:46, 31.13s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0003/0.2620:  91%|█████████ | 136/150 [1:10:33<07:16, 31.16s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2589:  91%|█████████ | 136/150 [1:11:04<07:16, 31.16s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2589:  91%|█████████▏| 137/150 [1:11:04<06:44, 31.14s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2648:  91%|█████████▏| 137/150 [1:11:35<06:44, 31.14s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2648:  92%|█████████▏| 138/150 [1:11:35<06:12, 31.06s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0004/0.2607:  92%|█████████▏| 138/150 [1:12:06<06:12, 31.06s/it]acc/rec/prec: 0.95/0.85/0.86  | train/valid loss: 0.0004/0.2607:  93%|█████████▎| 139/150 [1:12:06<05:41, 31.02s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2634:  93%|█████████▎| 139/150 [1:12:38<05:41, 31.02s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0003/0.2634:  93%|█████████▎| 140/150 [1:12:38<05:10, 31.08s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2620:  93%|█████████▎| 140/150 [1:13:09<05:10, 31.08s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2620:  94%|█████████▍| 141/150 [1:13:09<04:40, 31.16s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0004/0.2651:  94%|█████████▍| 141/150 [1:13:40<04:40, 31.16s/it]acc/rec/prec: 0.95/0.85/0.87  | train/valid loss: 0.0004/0.2651:  95%|█████████▍| 142/150 [1:13:40<04:09, 31.17s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2619:  95%|█████████▍| 142/150 [1:14:11<04:09, 31.17s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2619:  95%|█████████▌| 143/150 [1:14:11<03:38, 31.21s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2613:  95%|█████████▌| 143/150 [1:14:42<03:38, 31.21s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2613:  96%|█████████▌| 144/150 [1:14:42<03:07, 31.17s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2597:  96%|█████████▌| 144/150 [1:15:13<03:07, 31.17s/it]acc/rec/prec: 0.95/0.83/0.86  | train/valid loss: 0.0004/0.2597:  97%|█████████▋| 145/150 [1:15:13<02:35, 31.07s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2660:  97%|█████████▋| 145/150 [1:15:44<02:35, 31.07s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2660:  97%|█████████▋| 146/150 [1:15:44<02:04, 31.02s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2612:  97%|█████████▋| 146/150 [1:16:15<02:04, 31.02s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0004/0.2612:  98%|█████████▊| 147/150 [1:16:15<01:33, 31.02s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2615:  98%|█████████▊| 147/150 [1:16:46<01:33, 31.02s/it]acc/rec/prec: 0.95/0.83/0.87  | train/valid loss: 0.0003/0.2615:  99%|█████████▊| 148/150 [1:16:46<01:02, 31.07s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2626:  99%|█████████▊| 148/150 [1:17:17<01:02, 31.07s/it]acc/rec/prec: 0.95/0.84/0.87  | train/valid loss: 0.0003/0.2626:  99%|█████████▉| 149/150 [1:17:17<00:31, 31.07s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2636:  99%|█████████▉| 149/150 [1:17:48<00:31, 31.07s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2636: 100%|██████████| 150/150 [1:17:48<00:00, 31.03s/it]acc/rec/prec: 0.95/0.84/0.86  | train/valid loss: 0.0004/0.2636: 100%|██████████| 150/150 [1:17:48<00:00, 31.13s/it]
{'accuracy': {'abraham_grampa_simpson': 0.926701545715332, 'agnes_skinner': 0.6666666865348816, 'apu_nahasapeemapetilon': 0.9915966391563416, 'barney_gumble': 0.7727272510528564, 'bart_simpson': 0.9442379474639893, 'carl_carlson': 0.7692307829856873, 'charles_montgomery_burns': 0.9510204195976257, 'chief_wiggum': 0.9450549483299255, 'cletus_spuckler': 0.8571428656578064, 'comic_book_guy': 0.887499988079071, 'disco_stu': 1.0, 'edna_krabappel': 0.9108911156654358, 'fat_tony': 1.0, 'gil': 0.6666666865348816, 'groundskeeper_willie': 0.9166666865348816, 'homer_simpson': 0.9664429426193237, 'kent_brockman': 0.9895833134651184, 'krusty_the_clown': 0.9834710955619812, 'lenny_leonard': 0.9154929518699646, 'lionel_hutz': 0.0, 'lisa_simpson': 0.9372822046279907, 'maggie_simpson': 0.8148148059844971, 'marge_simpson': 0.9573643207550049, 'martin_prince': 0.7272727489471436, 'mayor_quimby': 0.8409090638160706, 'milhouse_van_houten': 0.9677419066429138, 'miss_hoover': 1.0, 'moe_szyslak': 0.9473684430122375, 'ned_flanders': 0.9749216437339783, 'nelson_muntz': 0.9137930870056152, 'otto_mann': 0.75, 'patty_bouvier': 0.9411764740943909, 'principal_skinner': 0.9820627570152283, 'professor_john_frink': 0.8571428656578064, 'rainier_wolfcastle': 0.7777777910232544, 'ralph_wiggum': 0.875, 'selma_bouvier': 0.9411764740943909, 'sideshow_bob': 0.9885057210922241, 'sideshow_mel': 0.8333333134651184, 'snake_jailbird': 0.5, 'troy_mcclure': 0.0, 'waylon_smithers': 0.8048780560493469}, 'recall': {'abraham_grampa_simpson': 0.926701545715332, 'agnes_skinner': 0.6666666865348816, 'apu_nahasapeemapetilon': 0.9915966391563416, 'barney_gumble': 0.7727272510528564, 'bart_simpson': 0.9442379474639893, 'carl_carlson': 0.7692307829856873, 'charles_montgomery_burns': 0.9510204195976257, 'chief_wiggum': 0.9450549483299255, 'cletus_spuckler': 0.8571428656578064, 'comic_book_guy': 0.887499988079071, 'disco_stu': 1.0, 'edna_krabappel': 0.9108911156654358, 'fat_tony': 1.0, 'gil': 0.6666666865348816, 'groundskeeper_willie': 0.9166666865348816, 'homer_simpson': 0.9664429426193237, 'kent_brockman': 0.9895833134651184, 'krusty_the_clown': 0.9834710955619812, 'lenny_leonard': 0.9154929518699646, 'lionel_hutz': 0.0, 'lisa_simpson': 0.9372822046279907, 'maggie_simpson': 0.8148148059844971, 'marge_simpson': 0.9573643207550049, 'martin_prince': 0.7272727489471436, 'mayor_quimby': 0.8409090638160706, 'milhouse_van_houten': 0.9677419066429138, 'miss_hoover': 1.0, 'moe_szyslak': 0.9473684430122375, 'ned_flanders': 0.9749216437339783, 'nelson_muntz': 0.9137930870056152, 'otto_mann': 0.75, 'patty_bouvier': 0.9411764740943909, 'principal_skinner': 0.9820627570152283, 'professor_john_frink': 0.8571428656578064, 'rainier_wolfcastle': 0.7777777910232544, 'ralph_wiggum': 0.875, 'selma_bouvier': 0.9411764740943909, 'sideshow_bob': 0.9885057210922241, 'sideshow_mel': 0.8333333134651184, 'snake_jailbird': 0.5, 'troy_mcclure': 0.0, 'waylon_smithers': 0.8048780560493469}, 'precision': {'abraham_grampa_simpson': 0.9779005646705627, 'agnes_skinner': 1.0, 'apu_nahasapeemapetilon': 0.9593495726585388, 'barney_gumble': 0.8500000238418579, 'bart_simpson': 0.9513108730316162, 'carl_carlson': 0.9523809552192688, 'charles_montgomery_burns': 0.9471544623374939, 'chief_wiggum': 0.9450549483299255, 'cletus_spuckler': 0.8571428656578064, 'comic_book_guy': 0.887499988079071, 'disco_stu': 0.6666666865348816, 'edna_krabappel': 0.9292929172515869, 'fat_tony': 1.0, 'gil': 0.800000011920929, 'groundskeeper_willie': 0.7857142686843872, 'homer_simpson': 0.9290322661399841, 'kent_brockman': 0.9595959782600403, 'krusty_the_clown': 0.9834710955619812, 'lenny_leonard': 0.9558823704719543, 'lionel_hutz': 0.0, 'lisa_simpson': 0.9572953581809998, 'maggie_simpson': 0.8148148059844971, 'marge_simpson': 0.946360170841217, 'martin_prince': 0.800000011920929, 'mayor_quimby': 0.8809523582458496, 'milhouse_van_houten': 0.9677419066429138, 'miss_hoover': 0.800000011920929, 'moe_szyslak': 0.9507042169570923, 'ned_flanders': 0.9779874086380005, 'nelson_muntz': 0.8153846263885498, 'otto_mann': 0.75, 'patty_bouvier': 0.8421052694320679, 'principal_skinner': 0.9563318490982056, 'professor_john_frink': 0.8571428656578064, 'rainier_wolfcastle': 1.0, 'ralph_wiggum': 0.875, 'selma_bouvier': 1.0, 'sideshow_bob': 0.9398906826972961, 'sideshow_mel': 1.0, 'snake_jailbird': 0.7777777910232544, 'troy_mcclure': 0.0, 'waylon_smithers': 1.0}}
