{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torchmetrics.classification as metrics\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torchinfo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from additonFunc import uniqufy_path, create_image_plot, save_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иницилизация ключевых значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAUNCH_NAME = \"MyUnet_FixedSet_default_augs\"\n",
    "LAUNCH_NAME = \"resnet50_megaSet_default_augs_lr\"\n",
    "\n",
    "\n",
    "STARTING_EPOCH = 0\n",
    "LOAD_WEIGHTS = None #\n",
    "LOAD_ADAM_STATE = None #\n",
    "USE_MANUAL_TENSORBOARD_FOLDER = None # \"/home/sega/progs/AI_Tasks/02_Task/TB_cache/MyUnet_FixedSet_skip_connections_1\" #\n",
    "\n",
    "SAVED_MODEL_PATH = None # \"/home/sega/progs/AI_Tasks/02_Task/TB_cache/MyUnet_FixedSet_skip_connections_1/weights_55.pth\" #\n",
    "\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1E-5 # 0.0001 #1E-5 for resnet-50\n",
    "WEIGHT_DECAY = 0 # 1E-7\n",
    "\n",
    "BATCH_SIZE = 96 # 20\n",
    "\n",
    "SAVE_METHOD = \"TORCH\" # \"TORCH\" / \"ONNX\"\n",
    "WEIGHT_SAVER = \"last\" # \"all\" / \"nothing\" / \"last\"\n",
    "\n",
    "CLASS_NAMES = ['other', 'road']\n",
    "CLASS_RGB_VALUES = [[0,0,0], [255, 255, 255]]\n",
    "\n",
    "NORMALIZE_MEAN_IMG =  [0.4295, 0.4325, 0.3961]       #[0.485, 0.456, 0.406]\n",
    "NORMALIZE_DEVIATIONS_IMG =  [0.2267, 0.2192, 0.2240] #[0.229, 0.224, 0.225]\n",
    " \n",
    "CROP_SIZE = (256, 256)\n",
    "\n",
    "NUM_WORKERS = 20\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATASET_DIR = '/usr/src/app/roads_dataset_cropped/tiff'\n",
    "VALID_SET   = (pjoin(DATASET_DIR, \"val\"), pjoin(DATASET_DIR, \"val_labels\"))\n",
    "TEST_SET   =  (pjoin(DATASET_DIR, \"test\"), pjoin(DATASET_DIR, \"test_labels\"))\n",
    "TRAIN_SET   = (pjoin(DATASET_DIR, \"train\"), pjoin(DATASET_DIR, \"train_labels\"))\n",
    "\n",
    "trained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBpath = uniqufy_path(f\"TB_cache/{LAUNCH_NAME}\") if USE_MANUAL_TENSORBOARD_FOLDER is None else USE_MANUAL_TENSORBOARD_FOLDER\n",
    "TBwriter = SummaryWriter(TBpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Класс содержащий разные методы для поворта изображения\n",
    "\"\"\"\n",
    "class RotationMethods():\n",
    "\n",
    "    __target_prefix = \"_RotationMethods__rotate_\"\n",
    "\n",
    "    def get_method(self, method_name):\n",
    "        methods = {key.removeprefix(self.__target_prefix):val for key, val in RotationMethods.__dict__.items() if key.startswith(self.__target_prefix)}\n",
    "        target_method = methods.get(method_name, None)\n",
    "        if target_method is None:\n",
    "            raise AttributeError(f\"Not found methods with name: '{method_name}'\\n\\tAvailable methods: {list(methods.keys())}\")\n",
    "        return target_method\n",
    "\n",
    "    @staticmethod\n",
    "    def __rotate_PIL(image, angle):\n",
    "        i = PImage.fromarray(image)\n",
    "        return np.array(i.rotate(angle, expand=True))\n",
    "\n",
    "    @staticmethod\n",
    "    def __rotate_rotMatrix(image, angle, scale = 1.0):\n",
    "        (h, w) = image.shape[:2]\n",
    "        center = (w//2, h//2)\n",
    "\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "\n",
    "        rotated_img = cv2.warpAffine(image, rotation_matrix, (w, h))\n",
    "\n",
    "        return rotated_img\n",
    "        \n",
    "    @staticmethod\n",
    "    def __rotate_manualImpl_1(image, angle):\n",
    "        (h, w) = image.shape[:2]\n",
    "        angle = 360-angle\n",
    "\n",
    "        angle_rad = np.deg2rad(angle)\n",
    "        sin = np.sin(angle_rad)\n",
    "        cos = np.cos(angle_rad)\n",
    "        \n",
    "        new_w = int(abs(h * sin) + abs(w * cos))\n",
    "        new_h = int(abs(h * cos) + abs(w * sin))\n",
    "\n",
    "        rotated_img = np.zeros((new_h, new_w, image.shape[2]), dtype=np.uint8)\n",
    "\n",
    "        center = (w//2, h//2)\n",
    "        new_center = (new_w // 2, new_h // 2)\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                new_x = j - new_center[0]\n",
    "                new_y = i - new_center[1]\n",
    "\n",
    "                x = int(new_x * cos + new_y * sin)\n",
    "                y = int(-new_x * sin + new_y * cos)\n",
    "\n",
    "                x += center[0]\n",
    "                y += center[1]\n",
    "\n",
    "                if 0 <= x < w and 0 <= y < h:\n",
    "                    rotated_img[i, j] = image[y, x]\n",
    "        return rotated_img\n",
    "    \n",
    "    @staticmethod\n",
    "    def __rotate_manualImpl_1vec1(image, angle):\n",
    "        (h, w) = image.shape[:2]\n",
    "        angle = np.deg2rad(360 - angle)\n",
    "\n",
    "        sin = np.sin(angle)\n",
    "        cos = np.cos(angle)\n",
    "\n",
    "        new_w = int(abs(h * sin) + abs(w * cos))\n",
    "        new_h = int(abs(h * cos) + abs(w * sin))\n",
    "\n",
    "        rotated_img = np.zeros((new_h, new_w, image.shape[2]), dtype=np.uint8)\n",
    "\n",
    "        center = np.array([w // 2, h // 2])\n",
    "        new_center = np.array([new_w // 2, new_h // 2])\n",
    "\n",
    "        new_x, new_y = np.meshgrid(np.arange(new_w), np.arange(new_h))\n",
    "        new_x = new_x - new_center[0]\n",
    "        new_y = new_y - new_center[1]\n",
    "\n",
    "        x = np.round(new_x * cos + new_y * sin).astype(int)\n",
    "        y = np.round(-new_x * sin + new_y * cos).astype(int)\n",
    "\n",
    "        x += center[0]\n",
    "        y += center[1]\n",
    "\n",
    "        valid_indices = np.logical_and.reduce((x >= 0, x < w, y >= 0, y < h))\n",
    "        rotated_img[new_y[valid_indices] + new_center[1], new_x[valid_indices] + new_center[0]] = image[y[valid_indices], x[valid_indices]]\n",
    "\n",
    "        return rotated_img\n",
    "        \n",
    "    @staticmethod\n",
    "    def __rotate_manualImpl_1vec2(image, angle):\n",
    "        (h, w) = image.shape[:2]\n",
    "        angle = np.deg2rad(360 - angle)\n",
    "\n",
    "        sin = np.sin(angle)\n",
    "        cos = np.cos(angle)\n",
    "\n",
    "        new_w = int(abs(h * sin) + abs(w * cos))\n",
    "        new_h = int(abs(h * cos) + abs(w * sin))\n",
    "\n",
    "        rotated_img = np.zeros((new_h, new_w, image.shape[2]), dtype=np.uint8)\n",
    "\n",
    "        center = np.array([w // 2, h // 2])\n",
    "        new_center = np.array([new_w // 2, new_h // 2])\n",
    "\n",
    "        new_x = np.arange(new_w) - new_center[0]\n",
    "        new_y = np.arange(new_h) - new_center[1]\n",
    "        new_x = new_x[:, np.newaxis]\n",
    "        new_y = new_y[np.newaxis, :]\n",
    "\n",
    "        x = np.round(new_x * cos + new_y * sin).astype(int)\n",
    "        y = np.round(-new_x * sin + new_y * cos).astype(int)\n",
    "\n",
    "        x += center[0]\n",
    "        y += center[1]\n",
    "\n",
    "        valid_indices = np.logical_and.reduce((x >= 0, x < w, y >= 0, y < h))\n",
    "        valid_indices_2d = np.nonzero(valid_indices)\n",
    "\n",
    "        valid_x = x[valid_indices]\n",
    "        valid_y = y[valid_indices]\n",
    "        valid_rotated_x = valid_indices_2d[1] + new_center[0]\n",
    "        valid_rotated_y = valid_indices_2d[0] + new_center[1]\n",
    "\n",
    "        mask = np.logical_and.reduce((valid_rotated_x >= 0, valid_rotated_x < new_w, valid_rotated_y >= 0, valid_rotated_y < new_h))\n",
    "        valid_rotated_x = valid_rotated_x[mask]\n",
    "        valid_rotated_y = valid_rotated_y[mask]\n",
    "        valid_x = valid_x[mask]\n",
    "        valid_y = valid_y[mask]\n",
    "\n",
    "        rotated_img[valid_rotated_y, valid_rotated_x] = image[valid_y, valid_x]\n",
    "\n",
    "\n",
    "        return rotated_img\n",
    "\n",
    "    @staticmethod\n",
    "    def __rotate_manualImpl_2(image, angle):\n",
    "        (h, w) = image.shape[:2]\n",
    "\n",
    "        angle_rad = np.deg2rad(angle)\n",
    "        sin = np.sin(angle_rad)\n",
    "        cos = np.cos(angle_rad)\n",
    "        \n",
    "        new_w = int(abs(h * sin) + abs(w * cos))\n",
    "        new_h = int(abs(h * cos) + abs(w * sin))\n",
    "\n",
    "        rotated_img = np.zeros((new_h, new_w, image.shape[2]), dtype=np.uint8)\n",
    "\n",
    "        center = (w//2, h//2)\n",
    "        new_center = (new_w // 2, new_h // 2)\n",
    "\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                x = j-center[0]\n",
    "                y = i-center[1]\n",
    "\n",
    "                new_x = x * cos + y * sin\n",
    "                new_y = -x * sin + y * cos\n",
    "\n",
    "                new_x = int(new_center[0] + new_x)\n",
    "                new_y = int(new_center[1] + new_y)\n",
    "\n",
    "                if 0 <= new_x < new_w and 0 <= new_y < new_h:\n",
    "                    rotated_img[new_y, new_x] = image[i,j]\n",
    "\n",
    "        return rotated_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRotater():\n",
    "\n",
    "    def __init__(self, mode : str = 'PIL', angle = None):\n",
    "        self.mode = mode\n",
    "        self.angle = angle\n",
    "        self.rotation_func = RotationMethods().get_method(mode)\n",
    "        \n",
    "    def __call__(self, image, angle = None, **kwargs):\n",
    "        if self.angle is not None:\n",
    "            angle = self.angle\n",
    "        return self.rotation_func(image, angle, **kwargs)\n",
    "\n",
    "    def forward(self, image, angle = None, **kwargs):\n",
    "        if self.angle is not None:\n",
    "            angle = self.angle\n",
    "        return self(image, angle, **kwargs)\n",
    "    \n",
    "    def apply(self, image, angle = None, **kwargs):\n",
    "        if self.angle is not None:\n",
    "            angle = self.angle\n",
    "        return self(image, angle, **kwargs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "prepare_to_network = A.Lambda(image=to_tensor, mask=to_tensor)\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.HorizontalFlip(p=1),\n",
    "                A.VerticalFlip(p=1),\n",
    "                A.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "        A.Normalize(mean=NORMALIZE_MEAN_IMG, std=NORMALIZE_DEVIATIONS_IMG, always_apply=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "valid_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=NORMALIZE_MEAN_IMG, std=NORMALIZE_DEVIATIONS_IMG, always_apply=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label, label_values):\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "    return semantic_map\n",
    "\n",
    "def reverse_one_hot(image):\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsDataset(Dataset):\n",
    "    def __init__(self, values_dir, labels_dir, class_rgb_values=None, transform=None, readyToNetwork=None):\n",
    "        self.values_dir = values_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.images = [pjoin(self.values_dir, filename) for filename in sorted(os.listdir(self.values_dir))]\n",
    "        self.labels = [pjoin(self.labels_dir, filename) for filename in sorted(os.listdir(self.labels_dir))]\n",
    "        self.transform = transform\n",
    "        self.readyToNetwork = readyToNetwork\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        label_path = self.labels[index]\n",
    "\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.cvtColor(cv2.imread(label_path), cv2.COLOR_BGR2RGB)\n",
    "        label = one_hot_encode(label, self.class_rgb_values).astype('float')\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(image=image, mask=label)\n",
    "            image, label = sample['image'], sample['mask']\n",
    "        if self.readyToNetwork:\n",
    "            sample = self.readyToNetwork(image=image, mask=label)\n",
    "            image, label = sample['image'], sample['mask']\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = RoadsDataset(*TEST_SET,\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform)\n",
    "\n",
    "for i in range(10):\n",
    "    image, mask = sample_dataset[np.random.randint(0, len(sample_dataset))]\n",
    "    TBwriter.add_figure(f'train samples', create_image_plot(origin=image, true=colour_code_segmentation(\n",
    "        reverse_one_hot(mask), CLASS_RGB_VALUES)), global_step=i)\n",
    "del(sample_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inC : int, outC : int, kernel_size, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        self.in_channels = inC\n",
    "        self.out_channels = outC\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv2d(inC, outC, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(outC)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class DeConvBlock(nn.Module):\n",
    "    def __init__(self, inC : int, outC : int, kernel_size, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        self.in_channels = inC\n",
    "        self.out_channels = outC\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.ConvTranspose2d(inC, outC, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(outC)\n",
    "        self.activation = nn.ReLU(True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inC, outC = 0, interC = -1, block_expansion = 4, skip_connection = True):\n",
    "        super().__init__()\n",
    "        self.block_expansion = block_expansion\n",
    "\n",
    "        self.downConv = None\n",
    "        self.upConv = None\n",
    "        self.skip_connection = None\n",
    "\n",
    "        if interC > 0:\n",
    "            self.downConv = ConvBlock(inC, interC, 1)\n",
    "        elif interC < 0:\n",
    "            if interC == -1:\n",
    "                interC = inC * self.block_expansion\n",
    "            else:\n",
    "                interC = inC * -interC\n",
    "            self.downConv = ConvBlock(inC, interC, 1)\n",
    "        else:\n",
    "            interC = inC\n",
    "\n",
    "        self.mainConv = ConvBlock(interC, interC, 3, padding=1, groups=interC)\n",
    "\n",
    "        if outC > 0:\n",
    "            self.upConv = ConvBlock(interC, outC, 1)\n",
    "        else:\n",
    "            outC = interC\n",
    "\n",
    "        if skip_connection:\n",
    "            self.skip_connection = ConvBlock(inC, outC, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inX = x.clone()\n",
    "        if self.downConv:\n",
    "            x = self.downConv(x)\n",
    "        x = self.mainConv(x)\n",
    "        if self.upConv:\n",
    "            x = self.upConv(x)\n",
    "        if self.skip_connection:\n",
    "            x = x + self.skip_connection(inX)\n",
    "        return x\n",
    "\n",
    "class ResidualStepBlock(nn.Module):\n",
    "    def __init__(self, inC, outC, global_block_expansion, interSize = 4, inter_block_expansion = 2, skip_connection = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip_connection = None\n",
    "\n",
    "        _innerConvs = []\n",
    "        previousC = inC\n",
    "        for stepC in range(inC, outC, (outC-inC)//interSize):\n",
    "            _innerConvs.append(ResidualBlock(previousC, stepC, block_expansion=inter_block_expansion))\n",
    "            previousC = stepC\n",
    "        _innerConvs.append(ResidualBlock(previousC, outC, block_expansion=inter_block_expansion))\n",
    "        \n",
    "        self.innerConvs = nn.Sequential(*_innerConvs)\n",
    "        if global_block_expansion > 0:\n",
    "            self.spatialConv = ConvBlock(inC, inC, 3, stride=global_block_expansion, padding=1)\n",
    "        else:\n",
    "            self.spatialConv = DeConvBlock(inC, inC, 4, stride=-global_block_expansion, padding=1)\n",
    "\n",
    "        if skip_connection:\n",
    "            self.skip_connection = ConvBlock(inC, outC, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.spatialConv(x)\n",
    "        resizedX = x.clone()\n",
    "        x = self.innerConvs(x)\n",
    "        if self.skip_connection:\n",
    "            x += self.skip_connection(resizedX)\n",
    "        return x\n",
    "\n",
    "class MyEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = ConvBlock(3, 64, 3, stride=4, padding=1)\n",
    "\n",
    "        self.conv_2_residual = ResidualStepBlock(64, 96, global_block_expansion=2, interSize=2)\n",
    "\n",
    "        self.conv_3_residual = ResidualStepBlock(96, 128, global_block_expansion=2, interSize=4)\n",
    "\n",
    "        self.conv_4_residual = ResidualStepBlock(128, 256, global_block_expansion=2, interSize=4)\n",
    "\n",
    "        self.conv_5_residual = ResidualStepBlock(256, 512, global_block_expansion=2, interSize=8)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = [x]\n",
    "        xs.append(self.conv_1(xs[-1]))\n",
    "        xs.append(self.conv_2_residual(xs[-1]))\n",
    "        xs.append(self.conv_3_residual(xs[-1]))\n",
    "        xs.append(self.conv_4_residual(xs[-1]))\n",
    "        xs.append(self.conv_5_residual(xs[-1]))\n",
    "        return xs\n",
    "\n",
    "class MyDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.deconv_1_residual = ResidualStepBlock(512, 256, global_block_expansion=-2, interSize=8)\n",
    "\n",
    "        self.deconv_2_residual = ResidualStepBlock(256, 128, global_block_expansion=-2, interSize=4)\n",
    "\n",
    "        self.deconv_3_residual = ResidualStepBlock(128, 96, global_block_expansion=-2, interSize=4)\n",
    "\n",
    "        self.deconv_4_residual = ResidualStepBlock(96, 64, global_block_expansion=-2, interSize=2)\n",
    "\n",
    "        self.deconv_5 = DeConvBlock(64, 3, 4, stride=4)\n",
    "    \n",
    "    def forward(self, encoder_samples):\n",
    "        x = self.deconv_1_residual(encoder_samples[-1]) + encoder_samples[-2]\n",
    "        x = self.deconv_2_residual(x) + encoder_samples[-3]\n",
    "        x = self.deconv_3_residual(x) + encoder_samples[-4]\n",
    "        x = self.deconv_4_residual(x) + encoder_samples[-5]\n",
    "        x = self.deconv_5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyUnet(nn.Module):\n",
    "    def __init__(self, outClasses : int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MyEncoder()\n",
    "        self.decoder = MyDecoder()\n",
    "\n",
    "        self.classificator = ConvBlock(3, outClasses, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_xs = self.encoder(x)\n",
    "        x = self.decoder(encoder_xs)\n",
    "        x = self.classificator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MyUnet(2)\n",
    "\n",
    "ENCODER = 'resnet50'\n",
    "CLASSES = CLASS_NAMES\n",
    "ACTIVATION = nn.ReLU\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    classes=len(CLASSES),\n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "# print(model_sum := torchinfo.summary(model, depth=3, input_size=(BATCH_SIZE, 3, *CROP_SIZE), row_settings=[\"var_names\"], verbose=0, col_names=[\n",
    "#       \"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\", \"trainable\"]))\n",
    "\n",
    "# dummy_input = torch.randn(1, 3, *CROP_SIZE)\n",
    "# torch.onnx.export(\n",
    "#             model.cpu(),\n",
    "#             dummy_input,\n",
    "#             \"model.onnx\",\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RoadsDataset(*TRAIN_SET,\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=train_transform, readyToNetwork=prepare_to_network)\n",
    "valid_dataset = RoadsDataset(*VALID_SET,\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform, readyToNetwork=prepare_to_network)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE//4,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================================================================================================================================================\n",
      "Layer (type (var_name))                            Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n",
      "=================================================================================================================================================================================================================================\n",
      "Unet (Unet)                                        [96, 3, 256, 256]         [96, 2, 256, 256]         --                             --                   --                        --                        True\n",
      "├─ResNetEncoder (encoder)                          [96, 3, 256, 256]         [96, 3, 256, 256]         --                             --                   --                        --                        True\n",
      "│    └─Conv2d (conv1)                              [96, 3, 256, 256]         [96, 64, 128, 128]        9,408                       0.03%                   [7, 7]                    14,797,504,512            True\n",
      "│    └─BatchNorm2d (bn1)                           [96, 64, 128, 128]        [96, 64, 128, 128]        128                         0.00%                   --                        12,288                    True\n",
      "│    └─ReLU (relu)                                 [96, 64, 128, 128]        [96, 64, 128, 128]        --                             --                   --                        --                        --\n",
      "│    └─MaxPool2d (maxpool)                         [96, 64, 128, 128]        [96, 64, 64, 64]          --                             --                   3                         --                        --\n",
      "│    └─Sequential (layer1)                         [96, 64, 64, 64]          [96, 256, 64, 64]         --                             --                   --                        --                        True\n",
      "│    │    └─Bottleneck (0)                         [96, 64, 64, 64]          [96, 256, 64, 64]         75,008                      0.23%                   --                        28,991,152,128            True\n",
      "│    │    └─Bottleneck (1)                         [96, 256, 64, 64]         [96, 256, 64, 64]         70,400                      0.22%                   --                        27,380,490,240            True\n",
      "│    │    └─Bottleneck (2)                         [96, 256, 64, 64]         [96, 256, 64, 64]         70,400                      0.22%                   --                        27,380,490,240            True\n",
      "│    └─Sequential (layer2)                         [96, 256, 64, 64]         [96, 512, 32, 32]         --                             --                   --                        --                        True\n",
      "│    │    └─Bottleneck (0)                         [96, 256, 64, 64]         [96, 512, 32, 32]         379,392                     1.17%                   --                        46,708,015,104            True\n",
      "│    │    └─Bottleneck (1)                         [96, 512, 32, 32]         [96, 512, 32, 32]         280,064                     0.86%                   --                        27,380,563,968            True\n",
      "│    │    └─Bottleneck (2)                         [96, 512, 32, 32]         [96, 512, 32, 32]         280,064                     0.86%                   --                        27,380,563,968            True\n",
      "│    │    └─Bottleneck (3)                         [96, 512, 32, 32]         [96, 512, 32, 32]         280,064                     0.86%                   --                        27,380,563,968            True\n",
      "│    └─Sequential (layer3)                         [96, 512, 32, 32]         [96, 1024, 16, 16]        --                             --                   --                        --                        True\n",
      "│    │    └─Bottleneck (0)                         [96, 512, 32, 32]         [96, 1024, 16, 16]        1,512,448                   4.65%                   --                        46,708,260,864            True\n",
      "│    │    └─Bottleneck (1)                         [96, 1024, 16, 16]        [96, 1024, 16, 16]        1,117,184                   3.44%                   --                        27,380,711,424            True\n",
      "│    │    └─Bottleneck (2)                         [96, 1024, 16, 16]        [96, 1024, 16, 16]        1,117,184                   3.44%                   --                        27,380,711,424            True\n",
      "│    │    └─Bottleneck (3)                         [96, 1024, 16, 16]        [96, 1024, 16, 16]        1,117,184                   3.44%                   --                        27,380,711,424            True\n",
      "│    │    └─Bottleneck (4)                         [96, 1024, 16, 16]        [96, 1024, 16, 16]        1,117,184                   3.44%                   --                        27,380,711,424            True\n",
      "│    │    └─Bottleneck (5)                         [96, 1024, 16, 16]        [96, 1024, 16, 16]        1,117,184                   3.44%                   --                        27,380,711,424            True\n",
      "│    └─Sequential (layer4)                         [96, 1024, 16, 16]        [96, 2048, 8, 8]          --                             --                   --                        --                        True\n",
      "│    │    └─Bottleneck (0)                         [96, 1024, 16, 16]        [96, 2048, 8, 8]          6,039,552                  18.57%                   --                        46,708,752,384            True\n",
      "│    │    └─Bottleneck (1)                         [96, 2048, 8, 8]          [96, 2048, 8, 8]          4,462,592                  13.72%                   --                        27,381,006,336            True\n",
      "│    │    └─Bottleneck (2)                         [96, 2048, 8, 8]          [96, 2048, 8, 8]          4,462,592                  13.72%                   --                        27,381,006,336            True\n",
      "├─UnetDecoder (decoder)                            [96, 3, 256, 256]         [96, 16, 256, 256]        --                             --                   --                        --                        True\n",
      "│    └─Identity (center)                           [96, 2048, 8, 8]          [96, 2048, 8, 8]          --                             --                   --                        --                        --\n",
      "│    └─ModuleList (blocks)                         --                        --                        --                             --                   --                        --                        True\n",
      "│    │    └─DecoderBlock (0)                       [96, 2048, 8, 8]          [96, 256, 16, 16]         7,668,736                  23.58%                   --                        188,441,788,416           True\n",
      "│    │    └─DecoderBlock (1)                       [96, 256, 16, 16]         [96, 128, 32, 32]         1,032,704                   3.18%                   --                        101,468,651,520           True\n",
      "│    │    └─DecoderBlock (2)                       [96, 128, 32, 32]         [96, 64, 64, 64]          258,304                     0.79%                   --                        101,468,626,944           True\n",
      "│    │    └─DecoderBlock (3)                       [96, 64, 64, 64]          [96, 32, 128, 128]        46,208                      0.14%                   --                        72,477,585,408            True\n",
      "│    │    └─DecoderBlock (4)                       [96, 32, 128, 128]        [96, 16, 256, 256]        6,976                       0.02%                   --                        43,486,550,016            True\n",
      "├─SegmentationHead (segmentation_head)             [96, 16, 256, 256]        [96, 2, 256, 256]         --                             --                   --                        --                        True\n",
      "│    └─Conv2d (0)                                  [96, 16, 256, 256]        [96, 2, 256, 256]         290                         0.00%                   [3, 3]                    1,824,522,240             True\n",
      "│    └─Identity (1)                                [96, 2, 256, 256]         [96, 2, 256, 256]         --                             --                   --                        --                        --\n",
      "│    └─Activation (2)                              [96, 2, 256, 256]         [96, 2, 256, 256]         --                             --                   --                        --                        --\n",
      "│    │    └─ReLU (activation)                      [96, 2, 256, 256]         [96, 2, 256, 256]         --                             --                   --                        --                        --\n",
      "=================================================================================================================================================================================================================================\n",
      "Total params: 32,521,250\n",
      "Trainable params: 32,521,250\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (T): 1.02\n",
      "=================================================================================================================================================================================================================================\n",
      "Input size (MB): 75.50\n",
      "Forward/backward pass size (MB): 28638.71\n",
      "Params size (MB): 130.09\n",
      "Estimated Total Size (MB): 28844.29\n",
      "=================================================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# images, _ = next(iter(valid_dataloader))\n",
    "# TBwriter.add_graph(model, images)\n",
    "\n",
    "if \"ONNX\" in SAVE_METHOD:\n",
    "    model_path = f\"{TBpath}/model_first.onnx\"\n",
    "    torch.onnx.export(model, torch.empty(size=(BATCH_SIZE, 3, *CROP_SIZE)), model_path)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(model_sum := torchinfo.summary(model, input_size=(BATCH_SIZE, 3, *CROP_SIZE), row_settings=[\"var_names\"], verbose=0, col_names=[\n",
    "      \"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\", \"trainable\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.losses.DiceLoss(mode='binary')\n",
    "\n",
    "optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=1E-6, weight_decay=WEIGHT_DECAY)\n",
    "optimizer_decoder = torch.optim.Adam([{'params':model.decoder.parameters()}, {'params':model.segmentation_head.parameters()}], lr=1E-3, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer_decoder, 'min', patience=3, threshold=1e-3, cooldown=1, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаги обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(net, criterion, optimizers, dataloader, epoch: int = None):\n",
    "    net.train()\n",
    "    running_loss = 0.\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.step()\n",
    "        running_loss += loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_loss = running_loss / len(dataloader)\n",
    "    return train_loss.item()\n",
    "\n",
    "\n",
    "def valid_step(net, criterion, dataloader, epoch: int = None):\n",
    "    net.eval()\n",
    "    running_loss = 0.\n",
    "    IoU = metrics.BinaryJaccardIndex()\n",
    "    IoU.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            output = net(images)\n",
    "\n",
    "            IoU(output, labels)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss\n",
    "\n",
    "            save_imgs(pjoin(TBpath, f\"valid_samples/samples_{epoch}\"), name=f\"img_{step}\",\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES))\n",
    "\n",
    "        TBwriter.add_figure('valid_sample', create_image_plot(\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES)),\n",
    "                  epoch)\n",
    "\n",
    "        valid_loss = running_loss / len(valid_dataloader)\n",
    "\n",
    "        return valid_loss.item(), IoU.compute().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = STARTING_EPOCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_WEIGHTS is not None:\n",
    "    model.state_dict(torch.load(LOAD_WEIGHTS))\n",
    "if LOAD_ADAM_STATE is not None:\n",
    "    optimizer.load_state_dict(torch.load(LOAD_ADAM_STATE))\n",
    "    \n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IoU: 0.93  | train/valid loss: 0.2099/0.2140:  33%|███▎      | 5/15 [22:33<45:08, 270.88s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] Saved weights with IoU: 0.93 | loss: 0.2140\n",
      "[5] Saved weights with IoU: 0.93 | loss: 0.2132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IoU: 0.93  | train/valid loss: 0.2096/0.2132:  40%|████      | 6/15 [27:05<40:39, 271.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] Saved weights with IoU: 0.93 | loss: 0.2129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IoU: 0.93  | train/valid loss: 0.2092/0.2129:  60%|██████    | 9/15 [40:38<27:06, 271.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9] Saved weights with IoU: 0.93 | loss: 0.2128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IoU: 0.93  | train/valid loss: 0.2091/0.2128:  67%|██████▋   | 10/15 [45:10<22:36, 271.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] Saved weights with IoU: 0.94 | loss: 0.2126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IoU: 0.94  | train/valid loss: 0.2090/0.2126:  73%|███████▎  | 11/15 [49:42<18:06, 271.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] Saved weights with IoU: 0.94 | loss: 0.2126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IoU: 0.94  | train/valid loss: 0.2089/0.2126:  80%|████████  | 12/15 [54:14<13:35, 271.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12] Saved weights with IoU: 0.94 | loss: 0.2125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IoU: 0.94  | train/valid loss: 0.2088/0.2125:  87%|████████▋ | 13/15 [58:46<09:03, 271.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13] Saved weights with IoU: 0.94 | loss: 0.2124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IoU: 0.94  | train/valid loss: 0.2087/0.2125: 100%|██████████| 15/15 [1:07:49<00:00, 271.73s/it]"
     ]
    }
   ],
   "source": [
    "best_loss = 10000\n",
    "trained = True\n",
    "\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "pbar.update(epoch)\n",
    "\n",
    "while(epoch < EPOCHS):\n",
    "    train_loss = train_step(model, loss, [optimizer_encoder, optimizer_decoder], train_dataloader, epoch)\n",
    "    valid_loss, iou_score = valid_step(model, loss, valid_dataloader, epoch)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    if WEIGHT_SAVER != \"nothing\" and valid_loss < best_loss and epoch > 3:\n",
    "        best_loss = valid_loss\n",
    "\n",
    "        print(f\"[{epoch}] Saved weights with IoU: {iou_score:.2f} | loss: {valid_loss:.4f}\")\n",
    "    \n",
    "        \n",
    "        if WEIGHT_SAVER == \"all\":\n",
    "            weights_path = f\"{TBpath}/weights_{epoch}.pth\"\n",
    "            model_path = f\"{TBpath}/model_{epoch}.onnx\"\n",
    "            optimizer_path = f\"{TBpath}/optimizer_{epoch}.pth\"\n",
    "            \n",
    "        elif WEIGHT_SAVER == \"last\":\n",
    "            weights_path = f\"{TBpath}/weights_last.pth\"\n",
    "            model_path =   f\"{TBpath}/model_last.onnx\"\n",
    "            optimizer_path = f\"{TBpath}/optimizer_last.pth\"\n",
    "\n",
    "        if \"TORCH\" in SAVE_METHOD:\n",
    "            torch.save(model.state_dict(), weights_path)\n",
    "        \n",
    "        if \"ONNX\" in SAVE_METHOD:\n",
    "            torch.onnx.export(model, torch.empty(size=(BATCH_SIZE, 3, *CROP_SIZE)), model_path)\n",
    "        \n",
    "#         torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "\n",
    "    TBwriter.add_scalar('valid loss', valid_loss, epoch)\n",
    "    TBwriter.add_scalar('train loss', train_loss, epoch)\n",
    "    \n",
    "    TBwriter.add_scalar('IoU', iou_score, epoch)\n",
    "\n",
    "    for ind, optimizer in enumerate([optimizer_encoder, optimizer_decoder]):\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            TBwriter.add_scalar(f'learning rate {ind}', float(param_group['lr']), epoch)\n",
    "\n",
    "    epoch += 1\n",
    "    pbar.update()\n",
    "    pbar.set_description(\n",
    "        f'IoU: {iou_score:.2f}  | train/valid loss: {train_loss:.4f}/{valid_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=NORMALIZE_MEAN_IMG, std=NORMALIZE_DEVIATIONS_IMG, always_apply=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = RoadsDataset(*TEST_SET,\n",
    "       class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform, readyToNetwork=prepare_to_network)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=36,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "if not trained:\n",
    "    print(f\"Используется не обученная модель, происходит загрузка модели из {LOAD_TESTSAVED_MODEL_PATH_WEIGHTS}\")\n",
    "    model = None\n",
    "    if \"ONNX\" in SAVE_METHOD and model is None:\n",
    "        print(f\"Попытка импорта модели из onnx файла\")\n",
    "        try:\n",
    "            import onnx\n",
    "            model = onnx.load(SAVED_MODEL_PATH)\n",
    "        except:\n",
    "            pass\n",
    "    if \"TORCH\" in SAVE_METHOD and model is None:\n",
    "        print(f\"Попытка импорта модели из pth файла\")\n",
    "        model = MyUnet(2)\n",
    "        #     model = smp.Unet(\n",
    "        #     encoder_name=ENCODER, \n",
    "        #     classes=len(CLASS_NAMES),\n",
    "        #     activation=ACTIVATION,\n",
    "        # )\n",
    "        model.state_dict(torch.load(f=SAVED_MODEL_PATH))\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_METRIC = metrics.MulticlassStatScores(num_classes=len(CLASS_NAMES), average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDivide(x, y): return torch.nan_to_num(x/y)\n",
    "\n",
    "def calculate_metric_by_errors(numerator, denominator, classes):\n",
    "    with torch.no_grad():\n",
    "        metric_values = saveDivide(numerator, denominator)\n",
    "        metric_per_class = {classname: val.item()\n",
    "                            for classname, val in zip(classes, metric_values)}\n",
    "        metric_average = torch.sum(metric_values)/len(classes)\n",
    "        metric_average_micro = saveDivide(torch.sum(numerator), torch.sum(denominator))\n",
    "        return (metric_values, metric_per_class, metric_average, metric_average_micro)\n",
    "\n",
    "def test_step(model, loader, metric : metrics.MulticlassStatScores):\n",
    "    classes = CLASS_NAMES\n",
    "    metric.to(DEVICE)\n",
    "\n",
    "    iou = metrics.JaccardIndex(task=\"multiclass\", num_classes=2).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for id, (images, labels) in enumerate(loader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            output = model(images)\n",
    "            TBwriter.add_figure('test_sample', create_image_plot(\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES)),\n",
    "                  id)\n",
    "            iou.update(output, labels)\n",
    "            metric.update(output, labels)\n",
    "\n",
    "    tp, fp, tn, fn = metric._final_state()\n",
    "\n",
    "    acc = calculate_metric_by_errors((tp+tn), (tp+fp+tn+fn), classes=classes)\n",
    "    rec = calculate_metric_by_errors(tp, (tp+fn), classes=classes)\n",
    "    prec = calculate_metric_by_errors(tp, (tp+fp), classes=classes)\n",
    "\n",
    "    jaccard = calculate_metric_by_errors(tp, (tp+fp+fn), classes=classes)\n",
    "    dice = calculate_metric_by_errors(2*tp, 2*tp+tn+tp, classes=classes)\n",
    "\n",
    "    metric_values = {\n",
    "        \"accuracy\": acc,\n",
    "        \"recall\": rec,\n",
    "        \"precision\": prec,\n",
    "        \"jaccard\": jaccard,\n",
    "        \"dice\": dice\n",
    "    }\n",
    "\n",
    "    return metric_values, iou.compute().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 2]' is invalid for input of size 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metric_values, iou \u001b[38;5;241m=\u001b[39m \u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEST_METRIC\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 31\u001b[0m, in \u001b[0;36mtest_step\u001b[0;34m(model, loader, metric)\u001b[0m\n\u001b[1;32m     23\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     24\u001b[0m         TBwriter\u001b[38;5;241m.\u001b[39madd_figure(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_sample\u001b[39m\u001b[38;5;124m'\u001b[39m, create_image_plot(\n\u001b[1;32m     25\u001b[0m             origin\u001b[38;5;241m=\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     26\u001b[0m             true\u001b[38;5;241m=\u001b[39mcolour_code_segmentation(reverse_one_hot(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)), CLASS_RGB_VALUES)),\n\u001b[1;32m     30\u001b[0m               \u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m         \u001b[43miou\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m         metric\u001b[38;5;241m.\u001b[39mupdate(output, labels)\n\u001b[1;32m     34\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39m_final_state()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchmetrics/metric.py:400\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    394\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m device corresponds to the device of the input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_on_cpu:\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_list_states_to_cpu()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchmetrics/metric.py:390\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchmetrics/classification/confusion_matrix.py:222\u001b[0m, in \u001b[0;36mMulticlassConfusionMatrix.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    220\u001b[0m     _multiclass_confusion_matrix_tensor_validation(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index)\n\u001b[1;32m    221\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_confusion_matrix_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index)\n\u001b[0;32m--> 222\u001b[0m confmat \u001b[38;5;241m=\u001b[39m \u001b[43m_multiclass_confusion_matrix_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfmat \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m confmat\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchmetrics/functional/classification/confusion_matrix.py:331\u001b[0m, in \u001b[0;36m_multiclass_confusion_matrix_update\u001b[0;34m(preds, target, num_classes)\u001b[0m\n\u001b[1;32m    329\u001b[0m unique_mapping \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;241m*\u001b[39m num_classes \u001b[38;5;241m+\u001b[39m preds\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    330\u001b[0m bins \u001b[38;5;241m=\u001b[39m _bincount(unique_mapping, minlength\u001b[38;5;241m=\u001b[39mnum_classes\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 2]' is invalid for input of size 64"
     ]
    }
   ],
   "source": [
    "metric_values, iou = test_step(model, valid_dataloader, TEST_METRIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric_values[\"jaccard\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_count = len(metric_values)\n",
    "\n",
    "fig, axes = plt.subplots(1, metric_count, figsize=(16,8))\n",
    "\n",
    "result_string = \"\"\n",
    "\n",
    "for (metricName, mValues), ax in zip(metric_values.items(), axes):\n",
    "    result_string += f\"\"\"{metricName}:\n",
    "    \\tmacro: {mValues[2]:.3f}\n",
    "    \\tmicro: {mValues[3]:.3f}\\n\"\"\"\n",
    "    mVal = mValues[1]\n",
    "    plt.sca(ax)\n",
    "    plt.bar(mVal.keys(), mVal.values())\n",
    "    plt.title(metricName)\n",
    "    plt.grid(axis='y')\n",
    "    plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "result_string += f\"\"\"jaccardIndex:\n",
    "macro: {iou}\"\"\"\n",
    "\n",
    "with open(f\"{TBpath}/save.txt\", 'w') as f:\n",
    "    f.write(result_string)\n",
    "\n",
    "print(result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IoU: 0.94  | train/valid loss: 0.2087/0.2125: 100%|██████████| 15/15 [1:22:39<00:00, 330.63s/it]"
     ]
    }
   ],
   "source": [
    "TBwriter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
