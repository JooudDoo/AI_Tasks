{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torchmetrics.classification as metrics\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torchinfo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from additonFunc import uniqufy_path, create_image_plot, save_imgs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иницилизация ключевых значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAUNCH_NAME = \"MyUnet_FixedSet\"\n",
    "\n",
    "STARTING_EPOCH = 0\n",
    "LOAD_WEIGHTS = None #\n",
    "LOAD_ADAM_STATE = None #\n",
    "USE_MANUAL_TENSORBOARD_FOLDER = None #\n",
    "\n",
    "LOAD_TEST_WEIGHTS = None # \"/home/sega/progs/AI_Tasks/02_Task/TB_cache/mobileNetV3_100large_1/weights_last.pth\" #\n",
    "\n",
    "EPOCHS = 60\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 1E-7\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "WEIGHT_SAVER = \"last\" # \"all\" / \"nothing\" / \"last\"\n",
    "\n",
    "CLASS_NAMES = ['other', 'road']\n",
    "CLASS_RGB_VALUES = [[0,0,0], [255, 255, 255]]\n",
    "\n",
    "NORMALIZE_MEAN_IMG = [0.4295, 0.4325, 0.3961]\n",
    "NORMALIZE_DEVIATIONS_IMG = [0.2267, 0.2192, 0.2240]\n",
    "\n",
    "CROP_SIZE = (256, 256)\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATASET_DIR = 'dataset/tiff'\n",
    "VALID_SET   = (pjoin(DATASET_DIR, \"val\"), pjoin(DATASET_DIR, \"val_labels\"))\n",
    "TEST_SET   =  (pjoin(DATASET_DIR, \"test\"), pjoin(DATASET_DIR, \"test_labels\"))\n",
    "TRAIN_SET   = (pjoin(DATASET_DIR, \"train\"), pjoin(DATASET_DIR, \"train_labels\"))\n",
    "\n",
    "trained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBpath = uniqufy_path(f\"TB_cache/{LAUNCH_NAME}\") if USE_MANUAL_TENSORBOARD_FOLDER is None else USE_MANUAL_TENSORBOARD_FOLDER\n",
    "TBwriter = SummaryWriter(TBpath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "prepare_to_network = A.Lambda(image=to_tensor, mask=to_tensor)\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(*CROP_SIZE, always_apply=True),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.HorizontalFlip(p=1),\n",
    "                A.VerticalFlip(p=1),\n",
    "                A.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "        A.Normalize(mean=NORMALIZE_MEAN_IMG, std=NORMALIZE_DEVIATIONS_IMG, always_apply=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "valid_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=NORMALIZE_MEAN_IMG, std=NORMALIZE_DEVIATIONS_IMG, always_apply=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label, label_values):\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "    return semantic_map\n",
    "\n",
    "def reverse_one_hot(image):\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsDataset(Dataset):\n",
    "    def __init__(self, values_dir, labels_dir, class_rgb_values=None, transform=None, readyToNetwork=None):\n",
    "        self.values_dir = values_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.images = [pjoin(self.values_dir, filename) for filename in sorted(os.listdir(self.values_dir))]\n",
    "        self.labels = [pjoin(self.labels_dir, filename) for filename in sorted(os.listdir(self.labels_dir))]\n",
    "        self.transform = transform\n",
    "        self.readyToNetwork = readyToNetwork\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        label_path = self.labels[index]\n",
    "\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.cvtColor(cv2.imread(label_path), cv2.COLOR_BGR2RGB)\n",
    "        label = one_hot_encode(label, self.class_rgb_values).astype('float')\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(image=image, mask=label)\n",
    "            image, label = sample['image'], sample['mask']\n",
    "        if self.readyToNetwork:\n",
    "            sample = self.readyToNetwork(image=image, mask=label)\n",
    "            image, label = sample['image'], sample['mask']\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = RoadsDataset(*TEST_SET,\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform)\n",
    "\n",
    "for i in range(10):\n",
    "    image, mask = sample_dataset[np.random.randint(0, len(sample_dataset))]\n",
    "    TBwriter.add_figure(f'train samples', create_image_plot(origin=image, true=colour_code_segmentation(\n",
    "        reverse_one_hot(mask), CLASS_RGB_VALUES)), global_step=i)\n",
    "del(sample_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inC : int, outC : int, kernel_size, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        self.in_channels = inC\n",
    "        self.out_channels = outC\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv2d(inC, outC, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(outC)\n",
    "        self.activation = nn.ReLU(True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "class DeConvBlock(nn.Module):\n",
    "    def __init__(self, inC : int, outC : int, kernel_size, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        self.in_channels = inC\n",
    "        self.out_channels = outC\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.ConvTranspose2d(inC, outC, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(outC)\n",
    "        self.activation = nn.ReLU(True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inC, outC = 0, interC = -1, block_expansion = 4):\n",
    "        super().__init__()\n",
    "        self.block_expansion = block_expansion\n",
    "\n",
    "        self.downConv = None\n",
    "        self.upConv = None\n",
    "\n",
    "        if interC > 0:\n",
    "            self.downConv = ConvBlock(inC, interC, 1)\n",
    "        elif interC < 0:\n",
    "            if interC == -1:\n",
    "                interC = inC * self.block_expansion\n",
    "            else:\n",
    "                interC = inC * -interC\n",
    "            self.downConv = ConvBlock(inC, interC, 1)\n",
    "        else:\n",
    "            interC = inC\n",
    "\n",
    "        self.mainConv = ConvBlock(interC, interC, 3, padding=1, groups=interC)\n",
    "\n",
    "        if outC > 0:\n",
    "            self.upConv = ConvBlock(interC, outC, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.downConv:\n",
    "            x = self.downConv(x)\n",
    "        x = self.mainConv(x)\n",
    "        if self.upConv:\n",
    "            x = self.upConv(x)\n",
    "        return x\n",
    "\n",
    "class ResidualStepBlock(nn.Module):\n",
    "    def __init__(self, inC, outC, global_block_expansion, interSize = 4, inter_block_expansion = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        _innerConvs = []\n",
    "        previousC = inC\n",
    "        for stepC in range(inC, outC, (outC-inC)//interSize):\n",
    "            _innerConvs.append(ResidualBlock(previousC, stepC, block_expansion=inter_block_expansion))\n",
    "            previousC = stepC\n",
    "        _innerConvs.append(ResidualBlock(previousC, outC, block_expansion=inter_block_expansion))\n",
    "        \n",
    "        self.innerConvs = nn.Sequential(*_innerConvs)\n",
    "        if global_block_expansion > 0:\n",
    "            self.spatialConv = ConvBlock(outC, outC, 3, stride=global_block_expansion, padding=1)\n",
    "        else:\n",
    "            self.spatialConv = DeConvBlock(outC, outC, 4, stride=-global_block_expansion, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.innerConvs(x)\n",
    "        x = self.spatialConv(x)\n",
    "        return x\n",
    "\n",
    "class MyEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = ConvBlock(3, 64, 3, stride=4, padding=1)\n",
    "\n",
    "        self.conv_2_residual = ResidualStepBlock(64, 96, global_block_expansion=2, interSize=2)\n",
    "\n",
    "        self.conv_3_residual = ResidualStepBlock(96, 128, global_block_expansion=2, interSize=4)\n",
    "\n",
    "        self.conv_4_residual = ResidualStepBlock(128, 256, global_block_expansion=2, interSize=4)\n",
    "\n",
    "        self.conv_5_residual = ResidualStepBlock(256, 512, global_block_expansion=2, interSize=8)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = [x]\n",
    "        xs.append(self.conv_1(xs[-1]))\n",
    "        xs.append(self.conv_2_residual(xs[-1]))\n",
    "        xs.append(self.conv_3_residual(xs[-1]))\n",
    "        xs.append(self.conv_4_residual(xs[-1]))\n",
    "        xs.append(self.conv_5_residual(xs[-1]))\n",
    "        return xs\n",
    "\n",
    "class MyDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.deconv_1_residual = ResidualStepBlock(512, 256, global_block_expansion=-2, interSize=8)\n",
    "\n",
    "        self.deconv_2_residual = ResidualStepBlock(256, 128, global_block_expansion=-2, interSize=4)\n",
    "\n",
    "        self.deconv_3_residual = ResidualStepBlock(128, 96, global_block_expansion=-2, interSize=4)\n",
    "\n",
    "        self.deconv_4_residual = ResidualStepBlock(96, 64, global_block_expansion=-2, interSize=2)\n",
    "\n",
    "        self.deconv_5 = DeConvBlock(64, 3, 4, stride=4)\n",
    "    \n",
    "    def forward(self, encoder_samples):\n",
    "        x = self.deconv_1_residual(encoder_samples[-1]) + encoder_samples[-2]\n",
    "        x = self.deconv_2_residual(x) + encoder_samples[-3]\n",
    "        x = self.deconv_3_residual(x) + encoder_samples[-4]\n",
    "        x = self.deconv_4_residual(x) + encoder_samples[-5]\n",
    "        x = self.deconv_5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyUnet(nn.Module):\n",
    "    def __init__(self, outClasses : int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MyEncoder()\n",
    "        self.decoder = MyDecoder()\n",
    "\n",
    "        self.classificator = ConvBlock(3, outClasses, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_xs = self.encoder(x)\n",
    "        x = self.decoder(encoder_xs)\n",
    "        x = self.classificator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyUnet(2)\n",
    "# ENCODER = 'timm-mobilenetv3_large_100'\n",
    "# CLASSES = CLASS_NAMES\n",
    "# ACTIVATION = nn.ReLU\n",
    "\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=ENCODER, \n",
    "#     classes=len(CLASSES),\n",
    "#     activation=ACTIVATION,\n",
    "# )\n",
    "\n",
    "# print(model_sum := torchinfo.summary(model, depth=3, input_size=(BATCH_SIZE, 3, *CROP_SIZE), row_settings=[\"var_names\"], verbose=0, col_names=[\n",
    "#       \"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\", \"trainable\"]))\n",
    "\n",
    "# dummy_input = torch.randn(1, 3, *CROP_SIZE)\n",
    "# torch.onnx.export(\n",
    "#             model.cpu(),\n",
    "#             dummy_input,\n",
    "#             \"model.onnx\",\n",
    "#         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RoadsDataset(*TRAIN_SET,\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=train_transform, readyToNetwork=prepare_to_network)\n",
    "valid_dataset = RoadsDataset(*VALID_SET,\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform, readyToNetwork=prepare_to_network)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================================================================================================================================================\n",
      "Layer (type (var_name))                            Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n",
      "=================================================================================================================================================================================================================================\n",
      "MyUnet (MyUnet)                                    [20, 3, 256, 256]         [20, 2, 256, 256]         --                             --                   --                        --                        True\n",
      "├─MyEncoder (encoder)                              [20, 3, 256, 256]         [20, 3, 256, 256]         --                             --                   --                        --                        True\n",
      "│    └─ConvBlock (conv_1)                          [20, 3, 256, 256]         [20, 64, 64, 64]          --                             --                   [3, 3]                    --                        True\n",
      "│    │    └─Conv2d (conv)                          [20, 3, 256, 256]         [20, 64, 64, 64]          1,792                       0.01%                   [3, 3]                    146,800,640               True\n",
      "│    │    └─BatchNorm2d (bn)                       [20, 64, 64, 64]          [20, 64, 64, 64]          128                         0.00%                   --                        2,560                     True\n",
      "│    │    └─ReLU (activation)                      [20, 64, 64, 64]          [20, 64, 64, 64]          --                             --                   --                        --                        --\n",
      "│    └─ResidualStepBlock (conv_2_residual)         [20, 64, 64, 64]          [20, 96, 32, 32]          --                             --                   --                        --                        True\n",
      "│    │    └─Sequential (innerConvs)                [20, 64, 64, 64]          [20, 96, 64, 64]          69,936                      0.38%                   --                        5,553,563,520             True\n",
      "│    │    └─ConvBlock (spatialConv)                [20, 96, 64, 64]          [20, 96, 32, 32]          83,232                      0.45%                   [3, 3]                    1,700,663,040             True\n",
      "│    └─ResidualStepBlock (conv_3_residual)         [20, 96, 32, 32]          [20, 128, 16, 16]         --                             --                   --                        --                        True\n",
      "│    │    └─Sequential (innerConvs)                [20, 96, 32, 32]          [20, 128, 32, 32]         249,200                     1.36%                   --                        4,994,277,760             True\n",
      "│    │    └─ConvBlock (spatialConv)                [20, 128, 32, 32]         [20, 128, 16, 16]         147,840                     0.81%                   [3, 3]                    755,635,200               True\n",
      "│    └─ResidualStepBlock (conv_4_residual)         [20, 128, 16, 16]         [20, 256, 8, 8]           --                             --                   --                        --                        True\n",
      "│    │    └─Sequential (innerConvs)                [20, 128, 16, 16]         [20, 256, 16, 16]         654,528                     3.56%                   --                        3,307,445,760             True\n",
      "│    │    └─ConvBlock (spatialConv)                [20, 256, 16, 16]         [20, 256, 8, 8]           590,592                     3.22%                   [3, 3]                    755,312,640               True\n",
      "│    └─ResidualStepBlock (conv_5_residual)         [20, 256, 8, 8]           [20, 512, 4, 4]           --                             --                   --                        --                        True\n",
      "│    │    └─Sequential (innerConvs)                [20, 256, 8, 8]           [20, 512, 8, 8]           5,062,528                  27.57%                   --                        6,439,070,720             True\n",
      "│    │    └─ConvBlock (spatialConv)                [20, 512, 8, 8]           [20, 512, 4, 4]           2,360,832                  12.86%                   [3, 3]                    755,159,040               True\n",
      "├─MyDecoder (decoder)                              [20, 3, 256, 256]         [20, 3, 256, 256]         --                             --                   --                        --                        True\n",
      "│    └─ResidualStepBlock (deconv_1_residual)       [20, 512, 4, 4]           [20, 256, 8, 8]           --                             --                   --                        --                        True\n",
      "│    │    └─Sequential (innerConvs)                [20, 512, 4, 4]           [20, 256, 4, 4]           6,257,536                  34.08%                   --                        1,991,429,120             True\n",
      "│    │    └─DeConvBlock (spatialConv)              [20, 256, 4, 4]           [20, 256, 8, 8]           1,049,344                   5.72%                   [4, 4]                    1,342,515,200             True\n",
      "│    └─ResidualStepBlock (deconv_2_residual)       [20, 256, 8, 8]           [20, 128, 16, 16]         --                             --                   --                        --                        True\n",
      "│    │    └─Sequential (innerConvs)                [20, 256, 8, 8]           [20, 128, 8, 8]           957,120                     5.21%                   --                        1,211,727,360             True\n",
      "│    │    └─DeConvBlock (spatialConv)              [20, 128, 8, 8]           [20, 128, 16, 16]         262,528                     1.43%                   [4, 4]                    1,342,837,760             True\n",
      "│    └─ResidualStepBlock (deconv_3_residual)       [20, 128, 16, 16]         [20, 96, 32, 32]          --                             --                   --                        --                        True\n",
      "│    │    └─Sequential (innerConvs)                [20, 128, 16, 16]         [20, 96, 16, 16]          294,128                     1.60%                   --                        1,476,069,760             True\n",
      "│    │    └─DeConvBlock (spatialConv)              [20, 96, 16, 16]          [20, 96, 32, 32]          147,744                     0.80%                   [4, 4]                    3,021,868,800             True\n",
      "│    └─ResidualStepBlock (deconv_4_residual)       [20, 96, 32, 32]          [20, 64, 64, 64]          --                             --                   --                        --                        True\n",
      "│    │    └─Sequential (innerConvs)                [20, 96, 32, 32]          [20, 64, 32, 32]          102,576                     0.56%                   --                        2,046,414,720             True\n",
      "│    │    └─DeConvBlock (spatialConv)              [20, 64, 32, 32]          [20, 64, 64, 64]          65,728                      0.36%                   [4, 4]                    5,373,954,560             True\n",
      "│    └─DeConvBlock (deconv_5)                      [20, 64, 64, 64]          [20, 3, 256, 256]         --                             --                   [4, 4]                    --                        True\n",
      "│    │    └─ConvTranspose2d (conv)                 [20, 64, 64, 64]          [20, 3, 256, 256]         3,075                       0.02%                   [4, 4]                    4,030,464,000             True\n",
      "│    │    └─BatchNorm2d (bn)                       [20, 3, 256, 256]         [20, 3, 256, 256]         6                           0.00%                   --                        120                       True\n",
      "│    │    └─ReLU (activation)                      [20, 3, 256, 256]         [20, 3, 256, 256]         --                             --                   --                        --                        --\n",
      "├─ConvBlock (classificator)                        [20, 3, 256, 256]         [20, 2, 256, 256]         --                             --                   [3, 3]                    --                        True\n",
      "│    └─Conv2d (conv)                               [20, 3, 256, 256]         [20, 2, 256, 256]         56                          0.00%                   [3, 3]                    73,400,320                True\n",
      "│    └─BatchNorm2d (bn)                            [20, 2, 256, 256]         [20, 2, 256, 256]         4                           0.00%                   --                        80                        True\n",
      "│    └─ReLU (activation)                           [20, 2, 256, 256]         [20, 2, 256, 256]         --                             --                   --                        --                        --\n",
      "=================================================================================================================================================================================================================================\n",
      "Total params: 18,360,453\n",
      "Trainable params: 18,360,453\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 46.32\n",
      "=================================================================================================================================================================================================================================\n",
      "Input size (MB): 15.73\n",
      "Forward/backward pass size (MB): 4212.00\n",
      "Params size (MB): 73.44\n",
      "Estimated Total Size (MB): 4301.17\n",
      "=================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sega/progs/AI_Tasks/AIenv/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/sega/progs/AI_Tasks/AIenv/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    }
   ],
   "source": [
    "# images, _ = next(iter(valid_dataloader))\n",
    "# TBwriter.add_graph(model, images)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(model_sum := torchinfo.summary(model, input_size=(BATCH_SIZE, 3, *CROP_SIZE), row_settings=[\"var_names\"], verbose=0, col_names=[\n",
    "      \"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\", \"trainable\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.losses.DiceLoss(mode='binary')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, threshold=1e-3, cooldown=1, factor=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаги обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(net, criterion, optimizer, dataloader, epoch: int = None):\n",
    "    net.train()\n",
    "    running_loss = 0.\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_loss = running_loss / len(dataloader)\n",
    "    return train_loss.item()\n",
    "\n",
    "\n",
    "def valid_step(net, criterion, dataloader, epoch: int = None):\n",
    "    net.eval()\n",
    "    running_loss = 0.\n",
    "    IoU = metrics.BinaryJaccardIndex()\n",
    "    IoU.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            output = net(images)\n",
    "\n",
    "            IoU(output, labels)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss\n",
    "\n",
    "            save_imgs(pjoin(TBpath, f\"valid_samples/samples_{epoch}\"), name=f\"img_{step}\",\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES))\n",
    "\n",
    "        TBwriter.add_figure('valid_sample', create_image_plot(\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES)),\n",
    "                  epoch)\n",
    "\n",
    "        valid_loss = running_loss / len(valid_dataloader)\n",
    "\n",
    "        return valid_loss.item(), IoU.compute().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = STARTING_EPOCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_WEIGHTS is not None:\n",
    "    model.state_dict(torch.load(LOAD_WEIGHTS))\n",
    "if LOAD_ADAM_STATE is not None:\n",
    "    optimizer.load_state_dict(torch.load(LOAD_ADAM_STATE))\n",
    "    \n",
    "None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0d0b2b743a48bcaccd457bb373bd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m pbar\u001b[39m.\u001b[39mupdate(epoch)\n\u001b[1;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m(epoch \u001b[39m<\u001b[39m EPOCHS):\n\u001b[0;32m----> 8\u001b[0m     train_loss \u001b[39m=\u001b[39m train_step(model, loss, optimizer, train_dataloader, epoch)\n\u001b[1;32m      9\u001b[0m     valid_loss, iou_score \u001b[39m=\u001b[39m valid_step(model, loss, valid_dataloader, epoch)\n\u001b[1;32m     10\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(valid_loss)\n",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(net, criterion, optimizer, dataloader, epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m----> 5\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39;49mto(DEVICE)\n\u001b[1;32m      6\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = 10000\n",
    "trained = True\n",
    "\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "pbar.update(epoch)\n",
    "\n",
    "while(epoch < EPOCHS):\n",
    "    train_loss = train_step(model, loss, optimizer, train_dataloader, epoch)\n",
    "    valid_loss, iou_score = valid_step(model, loss, valid_dataloader, epoch)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    if WEIGHT_SAVER != \"nothing\" and valid_loss < best_loss and epoch > 3:\n",
    "        best_loss = valid_loss\n",
    "\n",
    "        print(f\"[{epoch}] Saved weights with IoU: {iou_score:.2f} | loss: {valid_loss:.4f}\")\n",
    "    \n",
    "        if WEIGHT_SAVER == \"all\":\n",
    "            torch.save(optimizer.state_dict(), f\"{TBpath}/optimizer_{epoch}.pth\")\n",
    "            torch.save(model.state_dict(), f\"{TBpath}/weights_{epoch}.pth\")\n",
    "        elif WEIGHT_SAVER == \"last\":\n",
    "            torch.save(optimizer.state_dict(), f\"{TBpath}/optimizer_last.pth\")\n",
    "            torch.save(model.state_dict(), f\"{TBpath}/weights_last.pth\")\n",
    "\n",
    "    TBwriter.add_scalar('valid loss', valid_loss, epoch)\n",
    "    TBwriter.add_scalar('train loss', train_loss, epoch)\n",
    "    \n",
    "    TBwriter.add_scalar('IoU', iou_score, epoch)\n",
    "\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        TBwriter.add_scalar('learning rate', float(param_group['lr']), epoch)\n",
    "\n",
    "    epoch += 1\n",
    "    pbar.update()\n",
    "    pbar.set_description(\n",
    "        f'IoU: {iou_score:.2f}  | train/valid loss: {train_loss:.4f}/{valid_loss:.4f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=NORMALIZE_MEAN_IMG, std=NORMALIZE_DEVIATIONS_IMG, always_apply=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = RoadsDataset(*TEST_SET,\n",
    "       class_rgb_values=CLASS_RGB_VALUES, transform=test_transform, readyToNetwork=prepare_to_network)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "if not trained:\n",
    "    print(f\"Используется не обученая модель, происходит импорто весов из {LOAD_TEST_WEIGHTS}\")\n",
    "    model = MyUnet(2)\n",
    "#     model = smp.Unet(\n",
    "#     encoder_name=ENCODER, \n",
    "#     classes=len(CLASS_NAMES),\n",
    "#     activation=ACTIVATION,\n",
    "# )\n",
    "    model.state_dict(torch.load(f=LOAD_TEST_WEIGHTS))\n",
    "    model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_METRIC = metrics.MulticlassStatScores(num_classes=len(CLASS_NAMES), average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDivide(x, y): return torch.nan_to_num(x/y)\n",
    "\n",
    "def calculate_metric_by_errors(numerator, denominator, classes):\n",
    "    with torch.no_grad():\n",
    "        metric_values = saveDivide(numerator, denominator)\n",
    "        metric_per_class = {classname: val.item()\n",
    "                            for classname, val in zip(classes, metric_values)}\n",
    "        metric_average = torch.sum(metric_values)/len(classes)\n",
    "        metric_average_micro = saveDivide(torch.sum(numerator), torch.sum(denominator))\n",
    "        return (metric_values, metric_per_class, metric_average, metric_average_micro)\n",
    "\n",
    "def test_step(model, loader, metric : metrics.MulticlassStatScores):\n",
    "    classes = CLASS_NAMES\n",
    "    metric.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for id, (images, labels) in enumerate(loader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            output = model(images)\n",
    "            TBwriter.add_figure('test_sample', create_image_plot(\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES)),\n",
    "                  id)\n",
    "            metric.update(output, labels)\n",
    "\n",
    "    tp, fp, tn, fn = metric._final_state()\n",
    "\n",
    "    acc = calculate_metric_by_errors((tp+tn), (tp+fp+tn+fn), classes=classes)\n",
    "    rec = calculate_metric_by_errors(tp, (tp+fn), classes=classes)\n",
    "    prec = calculate_metric_by_errors(tp, (tp+fp), classes=classes)\n",
    "\n",
    "    jaccard = calculate_metric_by_errors(tp, (tp+fp+fn), classes=classes)\n",
    "    dice = calculate_metric_by_errors(2*tp, 2*tp+tn+tp, classes=classes)\n",
    "\n",
    "    metric_values = {\n",
    "        \"accuracy\": acc,\n",
    "        \"recall\": rec,\n",
    "        \"precision\": prec,\n",
    "        \"jaccard\": jaccard,\n",
    "        \"dice\": dice\n",
    "    }\n",
    "\n",
    "    return metric_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values = test_step(model, test_dataloader, TEST_METRIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric_values[\"jaccard\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_count = len(metric_values)\n",
    "\n",
    "fig, axes = plt.subplots(1, metric_count, figsize=(16,8))\n",
    "\n",
    "result_string = \"\"\n",
    "\n",
    "for (metricName, mValues), ax in zip(metric_values.items(), axes):\n",
    "    result_string += f\"\"\"{metricName}:\n",
    "    \\tmacro: {mValues[2]:.3f}\n",
    "    \\tmicro: {mValues[3]:.3f}\\n\"\"\"\n",
    "    mVal = mValues[1]\n",
    "    plt.sca(ax)\n",
    "    plt.bar(mVal.keys(), mVal.values())\n",
    "    plt.title(metricName)\n",
    "    plt.grid(axis='y')\n",
    "    plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBwriter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
