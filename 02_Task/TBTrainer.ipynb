{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torchmetrics.classification as metrics\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torchinfo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from additonFunc import uniqufy_path, create_image_plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иницилизация ключевых значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "WEIGHT_SAVER = \"last\" # \"all\" / \"nothing\" / \"last\"\n",
    "\n",
    "CLASS_NAMES = ['other', 'road']\n",
    "CLASS_RGB_VALUES = [[0,0,0], [255, 255, 255]]\n",
    "\n",
    "NORMALIZE_MEAN = (0, 0, 0)\n",
    "NORMALIZE_DEVIATIONS = (1, 1, 1)\n",
    "\n",
    "CROP_SIZE = (256, 256)\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TBwriter = SummaryWriter(uniqufy_path(\"TB_cache\\\\roads\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "prepare_to_network = A.Lambda(image=to_tensor, mask=to_tensor)\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(*CROP_SIZE, always_apply=True),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.HorizontalFlip(p=1),\n",
    "                A.VerticalFlip(p=1),\n",
    "                A.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "        A.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_DEVIATIONS),\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=cv2.BORDER_CONSTANT),\n",
    "        A.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_DEVIATIONS),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label, label_values):\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "    return semantic_map\n",
    "\n",
    "def reverse_one_hot(image):\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsDataset(Dataset):\n",
    "    def __init__(self, values_dir, labels_dir, class_rgb_values=None, transform=None, readyToNetwork=None):\n",
    "        self.values_dir = values_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.images = [pjoin(self.values_dir, filename) for filename in sorted(os.listdir(self.values_dir))]\n",
    "        self.labels = [pjoin(self.labels_dir, filename) for filename in sorted(os.listdir(self.labels_dir))]\n",
    "        self.transform = transform\n",
    "        self.readyToNetwork = readyToNetwork\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        label_path = self.labels[index]\n",
    "\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.cvtColor(cv2.imread(label_path), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "\n",
    "        label = one_hot_encode(label, self.class_rgb_values).astype('float')\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(image=image, mask=label)\n",
    "            image, label = sample['image'], sample['mask']\n",
    "        if self.readyToNetwork:\n",
    "            sample = self.readyToNetwork(image=image, mask=label)\n",
    "            image, label = sample['image'], sample['mask']\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = RoadsDataset(\"dataset\\\\tiff\\\\test\", \"dataset\\\\tiff\\\\test_labels\",\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=train_transform)\n",
    "\n",
    "for i in range(10):\n",
    "    image, mask = sample_dataset[np.random.randint(0, len(sample_dataset))]\n",
    "    TBwriter.add_figure(f'train samples', create_image_plot(origin=image, true=colour_code_segmentation(\n",
    "        reverse_one_hot(mask), CLASS_RGB_VALUES), one_hot=reverse_one_hot(mask)), global_step=i)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RoadsDataset(\"dataset\\\\tiff\\\\train\", \"dataset\\\\tiff\\\\train_labels\",\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=train_transform, readyToNetwork=prepare_to_network)\n",
    "valid_dataset = RoadsDataset(\"dataset\\\\tiff\\\\val\", \"dataset\\\\tiff\\\\val_labels\",\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform, readyToNetwork=prepare_to_network)\n",
    "test_dataset = RoadsDataset(\"dataset\\\\tiff\\\\test\", \"dataset\\\\tiff\\\\test_labels\",\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform, readyToNetwork=prepare_to_network)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Programming\\Python\\AI_Tasks\\AIenv\\lib\\site-packages\\segmentation_models_pytorch\\base\\model.py:16: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if h % output_stride != 0 or w % output_stride != 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================================================================================================================================================\n",
      "Layer (type (var_name))                            Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n",
      "=================================================================================================================================================================================================================================\n",
      "Unet (Unet)                                        [16, 3, 256, 256]         [16, 2, 256, 256]         --                             --                   --                        --                        True\n",
      "├─MobileNetV2Encoder (encoder)                     [16, 3, 256, 256]         [16, 3, 256, 256]         --                             --                   --                        --                        True\n",
      "│    └─Sequential (features)                       --                        --                        --                             --                   --                        --                        True\n",
      "│    │    └─Conv2dNormActivation (0)               [16, 3, 256, 256]         [16, 32, 128, 128]        928                         0.01%                   --                        226,493,440               True\n",
      "│    │    └─InvertedResidual (1)                   [16, 32, 128, 128]        [16, 16, 128, 128]        896                         0.01%                   --                        209,716,736               True\n",
      "│    │    └─InvertedResidual (2)                   [16, 16, 128, 128]        [16, 24, 64, 64]          5,136                       0.08%                   --                        610,278,144               True\n",
      "│    │    └─InvertedResidual (3)                   [16, 24, 64, 64]          [16, 24, 64, 64]          8,832                       0.13%                   --                        537,929,472               True\n",
      "│    │    └─InvertedResidual (4)                   [16, 24, 64, 64]          [16, 32, 32, 32]          10,000                      0.15%                   --                        323,233,792               True\n",
      "│    │    └─InvertedResidual (5)                   [16, 32, 32, 32]          [16, 32, 32, 32]          14,848                      0.22%                   --                        229,651,456               True\n",
      "│    │    └─InvertedResidual (6)                   [16, 32, 32, 32]          [16, 32, 32, 32]          14,848                      0.22%                   --                        229,651,456               True\n",
      "│    │    └─InvertedResidual (7)                   [16, 32, 32, 32]          [16, 64, 16, 16]          21,056                      0.32%                   --                        158,087,168               True\n",
      "│    │    └─InvertedResidual (8)                   [16, 64, 16, 16]          [16, 64, 16, 16]          54,272                      0.82%                   --                        215,508,992               True\n",
      "│    │    └─InvertedResidual (9)                   [16, 64, 16, 16]          [16, 64, 16, 16]          54,272                      0.82%                   --                        215,508,992               True\n",
      "│    │    └─InvertedResidual (10)                  [16, 64, 16, 16]          [16, 64, 16, 16]          54,272                      0.82%                   --                        215,508,992               True\n",
      "│    │    └─InvertedResidual (11)                  [16, 64, 16, 16]          [16, 96, 16, 16]          66,624                      1.01%                   --                        265,841,664               True\n",
      "│    │    └─InvertedResidual (12)                  [16, 96, 16, 16]          [16, 96, 16, 16]          118,272                     1.78%                   --                        474,258,432               True\n",
      "│    │    └─InvertedResidual (13)                  [16, 96, 16, 16]          [16, 96, 16, 16]          118,272                     1.78%                   --                        474,258,432               True\n",
      "│    │    └─InvertedResidual (14)                  [16, 96, 16, 16]          [16, 160, 8, 8]           155,264                     2.34%                   --                        326,214,656               True\n",
      "│    │    └─InvertedResidual (15)                  [16, 160, 8, 8]           [16, 160, 8, 8]           320,000                     4.83%                   --                        323,486,720               True\n",
      "│    │    └─InvertedResidual (16)                  [16, 160, 8, 8]           [16, 160, 8, 8]           320,000                     4.83%                   --                        323,486,720               True\n",
      "│    │    └─InvertedResidual (17)                  [16, 160, 8, 8]           [16, 320, 8, 8]           473,920                     7.15%                   --                        480,778,240               True\n",
      "│    │    └─Conv2dNormActivation (18)              [16, 320, 8, 8]           [16, 1280, 8, 8]          412,160                     6.22%                   --                        419,471,360               True\n",
      "├─UnetDecoder (decoder)                            [16, 3, 256, 256]         [16, 16, 256, 256]        --                             --                   --                        --                        True\n",
      "│    └─Identity (center)                           [16, 1280, 8, 8]          [16, 1280, 8, 8]          --                             --                   --                        --                        --\n",
      "│    └─ModuleList (blocks)                         --                        --                        --                             --                   --                        --                        True\n",
      "│    │    └─DecoderBlock (0)                       [16, 1280, 8, 8]          [16, 256, 16, 16]         3,761,152                  56.74%                   --                        15,401,500,672            True\n",
      "│    │    └─DecoderBlock (1)                       [16, 256, 16, 16]         [16, 128, 32, 32]         479,744                     7.24%                   --                        7,851,745,280             True\n",
      "│    │    └─DecoderBlock (2)                       [16, 128, 32, 32]         [16, 64, 64, 64]          124,672                     1.88%                   --                        8,153,731,072             True\n",
      "│    │    └─DecoderBlock (3)                       [16, 64, 64, 64]          [16, 32, 128, 128]        32,384                      0.49%                   --                        8,455,718,912             True\n",
      "│    │    └─DecoderBlock (4)                       [16, 32, 128, 128]        [16, 16, 256, 256]        6,976                       0.11%                   --                        7,247,758,336             True\n",
      "├─SegmentationHead (segmentation_head)             [16, 16, 256, 256]        [16, 2, 256, 256]         --                             --                   --                        --                        True\n",
      "│    └─Conv2d (0)                                  [16, 16, 256, 256]        [16, 2, 256, 256]         290                         0.00%                   [3, 3]                    304,087,040               True\n",
      "│    └─Identity (1)                                [16, 2, 256, 256]         [16, 2, 256, 256]         --                             --                   --                        --                        --\n",
      "│    └─Activation (2)                              [16, 2, 256, 256]         [16, 2, 256, 256]         --                             --                   --                        --                        --\n",
      "│    │    └─Sigmoid (activation)                   [16, 2, 256, 256]         [16, 2, 256, 256]         --                             --                   --                        --                        --\n",
      "=================================================================================================================================================================================================================================\n",
      "Total params: 6,629,090\n",
      "Trainable params: 6,629,090\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 53.67\n",
      "=================================================================================================================================================================================================================================\n",
      "Input size (MB): 12.58\n",
      "Forward/backward pass size (MB): 3289.91\n",
      "Params size (MB): 26.52\n",
      "Estimated Total Size (MB): 3329.01\n",
      "=================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Programming\\Python\\AI_Tasks\\AIenv\\lib\\site-packages\\torchinfo\\torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "e:\\Programming\\Python\\AI_Tasks\\AIenv\\lib\\site-packages\\torch\\storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    }
   ],
   "source": [
    "ENCODER = 'mobilenet_v2'\n",
    "CLASSES = CLASS_NAMES\n",
    "ACTIVATION = \"sigmoid\"\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "images, _ = next(iter(test_dataloader))\n",
    "TBwriter.add_graph(model, images)\n",
    "\n",
    "# torch.onnx.export(model,\n",
    "#                   images,\n",
    "#                   \"model.onnx\")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(model_sum := torchinfo.summary(model, input_size=(BATCH_SIZE, 3, *CROP_SIZE), row_settings=[\"var_names\"], verbose=0, col_names=[\n",
    "      \"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\", \"trainable\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.losses.DiceLoss(mode='binary')\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTuples(a1 : tuple, a2 : tuple):\n",
    "    for i in range(len(a1)):\n",
    "        a1[i] += a2[i]\n",
    "    return a1\n",
    "    \n",
    "\n",
    "def train_step(net, criterion, optimizer, dataloader, epoch : int = None):\n",
    "    net.train()\n",
    "    running_loss = 0.\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "    return train_loss.item()\n",
    "\n",
    "def valid_step(net, criterion, dataloader, epoch : int = None):\n",
    "    net.eval()\n",
    "    running_loss = 0.\n",
    "    IoU = metrics.BinaryJaccardIndex()\n",
    "    IoU.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            output = net(images)\n",
    "\n",
    "            IoU(output, labels)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss\n",
    "\n",
    "        valid_loss = running_loss / len(valid_dataloader)\n",
    "\n",
    "        return valid_loss.item(), IoU.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b652ef2f68b4d23a017ae8e6765ee9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_loss = 10000\n",
    "trained = True\n",
    "\n",
    "for epoch in (pbar := tqdm(range(EPOCHS))):\n",
    "    train_loss = train_step(model, loss, optimizer, train_dataloader, epoch)\n",
    "    valid_loss, iou_score = valid_step(model, loss, valid_dataloader, epoch)\n",
    "\n",
    "    if WEIGHT_SAVER != \"nothing\" and valid_loss < best_loss and epoch > 3:\n",
    "        best_loss = valid_loss\n",
    "\n",
    "        print(f\"Saved weights with IoU: {iou_score:.2f} | loss: {valid_loss:.4f}\")\n",
    "    \n",
    "        if WEIGHT_SAVER == \"all\":\n",
    "            torch.save(model.state_dict(),\n",
    "                       f\"weights_{epoch}.pth\")\n",
    "        elif WEIGHT_SAVER == \"last\":\n",
    "            torch.save(model.state_dict(),\n",
    "                       f\"weights_last.pth\")\n",
    "\n",
    "    TBwriter.add_scalar('valid loss', valid_loss, epoch)\n",
    "    TBwriter.add_scalar('train loss', train_loss, epoch)\n",
    "    \n",
    "    TBwriter.add_scalar('IoU', iou_score, epoch)\n",
    "\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        TBwriter.add_scalar('learning rate', float(param_group['lr']), epoch)\n",
    "\n",
    "    pbar.set_description(\n",
    "        f'IoU: {iou_score:.2f}  | train/valid loss: {train_loss:.4f}/{valid_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBwriter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
