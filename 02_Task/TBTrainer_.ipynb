{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torchmetrics.classification as metrics\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torchinfo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from additonFunc import uniqufy_path, create_image_plot, save_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иницилизация ключевых значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAUNCH_NAME = \"MyUnet_FixedSet_augmented\"\n",
    "# LAUNCH_NAME = \"resnet50_megaSet\"\n",
    "\n",
    "\n",
    "STARTING_EPOCH = 0\n",
    "LOAD_WEIGHTS = None #\n",
    "LOAD_ADAM_STATE = None #\n",
    "USE_MANUAL_TENSORBOARD_FOLDER = \"/home/sega/progs/AI_Tasks/02_Task/TB_cache/MyUnet_Fixedset_augmented\" #\n",
    "\n",
    "SAVED_MODEL_PATH = \"/home/sega/progs/AI_Tasks/02_Task/TB_cache/MyUnet_Fixedset_augmented/weights_last.pth\" #\n",
    "\n",
    "EPOCHS = 60\n",
    "LEARNING_RATE = 0.0001 #1E-5 for resnet-50\n",
    "WEIGHT_DECAY = 1E-7\n",
    "\n",
    "# другие loss\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "SAVE_METHOD = \"TORCH_ONNX\" # \"TORCH\" / \"ONNX\"\n",
    "WEIGHT_SAVER = \"last\" # \"all\" / \"nothing\" / \"last\"\n",
    "\n",
    "CLASS_NAMES = ['other', 'road']\n",
    "CLASS_RGB_VALUES = [[0,0,0], [255, 255, 255]]\n",
    "\n",
    "NORMALIZE_MEAN_IMG =  [0.4295, 0.4325, 0.3961]       #[0.485, 0.456, 0.406]\n",
    "NORMALIZE_DEVIATIONS_IMG =  [0.2267, 0.2192, 0.2240] #[0.229, 0.224, 0.225]\n",
    " \n",
    "CROP_SIZE = (256, 256)\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATASET_DIR ='dataset/tiff'\n",
    "VALID_SET   = (pjoin(DATASET_DIR, \"val\"), pjoin(DATASET_DIR, \"val_labels\"))\n",
    "TEST_SET    = (pjoin(DATASET_DIR, \"test\"), pjoin(DATASET_DIR, \"test_labels\"))\n",
    "TRAIN_SET   = (pjoin(DATASET_DIR, \"train\"), pjoin(DATASET_DIR, \"train_labels\"))\n",
    "\n",
    "trained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBpath = uniqufy_path(f\"TB_cache/{LAUNCH_NAME}\") if USE_MANUAL_TENSORBOARD_FOLDER is None else USE_MANUAL_TENSORBOARD_FOLDER\n",
    "TBwriter = SummaryWriter(TBpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "prepare_to_network = A.Lambda(image=to_tensor, mask=to_tensor)\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.HorizontalFlip(p=1),\n",
    "                A.VerticalFlip(p=1),\n",
    "                A.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "        A.Normalize(mean=NORMALIZE_MEAN_IMG, std=NORMALIZE_DEVIATIONS_IMG, always_apply=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "valid_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=NORMALIZE_MEAN_IMG, std=NORMALIZE_DEVIATIONS_IMG, always_apply=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label, label_values):\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "    return semantic_map\n",
    "\n",
    "def reverse_one_hot(image):\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsDataset(Dataset):\n",
    "    def __init__(self, values_dir, labels_dir, class_rgb_values=None, transform=None, readyToNetwork=None):\n",
    "        self.values_dir = values_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.images = [pjoin(self.values_dir, filename) for filename in sorted(os.listdir(self.values_dir))]\n",
    "        self.labels = [pjoin(self.labels_dir, filename) for filename in sorted(os.listdir(self.labels_dir))]\n",
    "        self.transform = transform\n",
    "        self.readyToNetwork = readyToNetwork\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        label_path = self.labels[index]\n",
    "\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.cvtColor(cv2.imread(label_path), cv2.COLOR_BGR2RGB)\n",
    "        label = one_hot_encode(label, self.class_rgb_values).astype('float')\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(image=image, mask=label)\n",
    "            image, label = sample['image'], sample['mask']\n",
    "        if self.readyToNetwork:\n",
    "            sample = self.readyToNetwork(image=image, mask=label)\n",
    "            image, label = sample['image'], sample['mask']\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = RoadsDataset(*TEST_SET,\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform)\n",
    "\n",
    "for i in range(10):\n",
    "    image, mask = sample_dataset[np.random.randint(0, len(sample_dataset))]\n",
    "    TBwriter.add_figure(f'train samples', create_image_plot(origin=image, true=colour_code_segmentation(\n",
    "        reverse_one_hot(mask), CLASS_RGB_VALUES)), global_step=i)\n",
    "del(sample_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inC : int, outC : int, kernel_size, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        self.in_channels = inC\n",
    "        self.out_channels = outC\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.Conv2d(inC, outC, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(outC)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class DeConvBlock(nn.Module):\n",
    "    def __init__(self, inC : int, outC : int, kernel_size, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        self.in_channels = inC\n",
    "        self.out_channels = outC\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = nn.ConvTranspose2d(inC, outC, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(outC)\n",
    "        self.activation = nn.ReLU(True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inC, outC = 0, interC = -1, block_expansion = 4, skip_connection = True):\n",
    "        super().__init__()\n",
    "        self.block_expansion = block_expansion\n",
    "\n",
    "        self.downConv = None\n",
    "        self.upConv = None\n",
    "        self.skip_connection = None\n",
    "\n",
    "        if interC > 0:\n",
    "            self.downConv = ConvBlock(inC, interC, 1)\n",
    "        elif interC < 0:\n",
    "            if interC == -1:\n",
    "                interC = inC * self.block_expansion\n",
    "            else:\n",
    "                interC = inC * -interC\n",
    "            self.downConv = ConvBlock(inC, interC, 1)\n",
    "        else:\n",
    "            interC = inC\n",
    "\n",
    "        self.mainConv = ConvBlock(interC, interC, 3, padding=1, groups=interC)\n",
    "\n",
    "        if outC > 0:\n",
    "            self.upConv = ConvBlock(interC, outC, 1)\n",
    "        else:\n",
    "            outC = interC\n",
    "\n",
    "        if skip_connection:\n",
    "            self.skip_connection = ConvBlock(inC, outC, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inX = x.clone()\n",
    "        if self.downConv:\n",
    "            x = self.downConv(x)\n",
    "        x = self.mainConv(x)\n",
    "        if self.upConv:\n",
    "            x = self.upConv(x)\n",
    "        if self.skip_connection:\n",
    "            x = x + self.skip_connection(inX)\n",
    "        return x\n",
    "\n",
    "class ResidualStepBlock(nn.Module):\n",
    "    def __init__(self, inC, outC, global_block_expansion, interSize = 4, inter_block_expansion = 2, skip_connection = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip_connection = None\n",
    "\n",
    "        _innerConvs = []\n",
    "        previousC = inC\n",
    "        for stepC in range(inC, outC, (outC-inC)//interSize):\n",
    "            _innerConvs.append(ResidualBlock(previousC, stepC, block_expansion=inter_block_expansion))\n",
    "            previousC = stepC\n",
    "        _innerConvs.append(ResidualBlock(previousC, outC, block_expansion=inter_block_expansion))\n",
    "        \n",
    "        self.innerConvs = nn.Sequential(*_innerConvs)\n",
    "        if global_block_expansion > 0:\n",
    "            self.spatialConv = ConvBlock(inC, inC, 3, stride=global_block_expansion, padding=1)\n",
    "        else:\n",
    "            self.spatialConv = DeConvBlock(inC, inC, 4, stride=-global_block_expansion, padding=1)\n",
    "\n",
    "        if skip_connection:\n",
    "            self.skip_connection = ConvBlock(inC, outC, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.spatialConv(x)\n",
    "        resizedX = x.clone()\n",
    "        x = self.innerConvs(x)\n",
    "        if self.skip_connection:\n",
    "            x += self.skip_connection(resizedX)\n",
    "        return x\n",
    "\n",
    "class MyEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = ConvBlock(3, 64, 3, stride=4, padding=1)\n",
    "\n",
    "        self.conv_2_residual = ResidualStepBlock(64, 96, global_block_expansion=2, interSize=2)\n",
    "\n",
    "        self.conv_3_residual = ResidualStepBlock(96, 128, global_block_expansion=2, interSize=4)\n",
    "\n",
    "        self.conv_4_residual = ResidualStepBlock(128, 256, global_block_expansion=2, interSize=4)\n",
    "\n",
    "        self.conv_5_residual = ResidualStepBlock(256, 512, global_block_expansion=2, interSize=8)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = [x]\n",
    "        xs.append(self.conv_1(xs[-1]))\n",
    "        xs.append(self.conv_2_residual(xs[-1]))\n",
    "        xs.append(self.conv_3_residual(xs[-1]))\n",
    "        xs.append(self.conv_4_residual(xs[-1]))\n",
    "        xs.append(self.conv_5_residual(xs[-1]))\n",
    "        return xs\n",
    "\n",
    "class MyDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.deconv_1_residual = ResidualStepBlock(512, 256, global_block_expansion=-2, interSize=8)\n",
    "\n",
    "        self.deconv_2_residual = ResidualStepBlock(256, 128, global_block_expansion=-2, interSize=4)\n",
    "\n",
    "        self.deconv_3_residual = ResidualStepBlock(128, 96, global_block_expansion=-2, interSize=4)\n",
    "\n",
    "        self.deconv_4_residual = ResidualStepBlock(96, 64, global_block_expansion=-2, interSize=2)\n",
    "\n",
    "        self.deconv_5 = DeConvBlock(64, 3, 4, stride=4)\n",
    "    \n",
    "    def forward(self, encoder_samples):\n",
    "        x = self.deconv_1_residual(encoder_samples[-1]) + encoder_samples[-2]\n",
    "        x = self.deconv_2_residual(x) + encoder_samples[-3]\n",
    "        x = self.deconv_3_residual(x) + encoder_samples[-4]\n",
    "        x = self.deconv_4_residual(x) + encoder_samples[-5]\n",
    "        x = self.deconv_5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyUnet(nn.Module):\n",
    "    def __init__(self, outClasses : int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MyEncoder()\n",
    "        self.decoder = MyDecoder()\n",
    "\n",
    "        self.classificator = ConvBlock(3, outClasses, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_xs = self.encoder(x)\n",
    "        x = self.decoder(encoder_xs)\n",
    "        x = self.classificator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7ff9f0922c70>\n"
     ]
    }
   ],
   "source": [
    "# model = MyUnet(2)\n",
    "\n",
    "ENCODER = 'resnet50'\n",
    "CLASSES = CLASS_NAMES\n",
    "ACTIVATION = nn.ReLU\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    classes=len(CLASSES),\n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "# print(model_sum := torchinfo.summary(model, depth=3, input_size=(BATCH_SIZE, 3, *CROP_SIZE), row_settings=[\"var_names\"], verbose=0, col_names=[\n",
    "#       \"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\", \"trainable\"]))\n",
    "\n",
    "# dummy_input = torch.randn(1, 3, *CROP_SIZE)\n",
    "# torch.onnx.export(\n",
    "#             model.cpu(),\n",
    "#             dummy_input,\n",
    "#             \"model.onnx\",\n",
    "#         )\n",
    "\n",
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RoadsDataset(*TRAIN_SET,\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=train_transform, readyToNetwork=prepare_to_network)\n",
    "valid_dataset = RoadsDataset(*VALID_SET,\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform, readyToNetwork=prepare_to_network)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE//4,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sega/progs/AI_Tasks/AIenv/lib/python3.10/site-packages/segmentation_models_pytorch/base/model.py:16: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if h % output_stride != 0 or w % output_stride != 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "=================================================================================================================================================================================================================================\n",
      "Layer (type (var_name))                            Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n",
      "=================================================================================================================================================================================================================================\n",
      "Unet (Unet)                                        [20, 3, 256, 256]         [20, 2, 256, 256]         --                             --                   --                        --                        True\n",
      "├─ResNetEncoder (encoder)                          [20, 3, 256, 256]         [20, 3, 256, 256]         --                             --                   --                        --                        True\n",
      "│    └─Conv2d (conv1)                              [20, 3, 256, 256]         [20, 64, 128, 128]        9,408                       0.03%                   [7, 7]                    3,082,813,440             True\n",
      "│    └─BatchNorm2d (bn1)                           [20, 64, 128, 128]        [20, 64, 128, 128]        128                         0.00%                   --                        2,560                     True\n",
      "│    └─ReLU (relu)                                 [20, 64, 128, 128]        [20, 64, 128, 128]        --                             --                   --                        --                        --\n",
      "│    └─MaxPool2d (maxpool)                         [20, 64, 128, 128]        [20, 64, 64, 64]          --                             --                   3                         --                        --\n",
      "│    └─Sequential (layer1)                         [20, 64, 64, 64]          [20, 256, 64, 64]         --                             --                   --                        --                        True\n",
      "│    │    └─Bottleneck (0)                         [20, 64, 64, 64]          [20, 256, 64, 64]         75,008                      0.23%                   --                        6,039,823,360             True\n",
      "│    │    └─Bottleneck (1)                         [20, 256, 64, 64]         [20, 256, 64, 64]         70,400                      0.22%                   --                        5,704,268,800             True\n",
      "│    │    └─Bottleneck (2)                         [20, 256, 64, 64]         [20, 256, 64, 64]         70,400                      0.22%                   --                        5,704,268,800             True\n",
      "│    └─Sequential (layer2)                         [20, 256, 64, 64]         [20, 512, 32, 32]         --                             --                   --                        --                        True\n",
      "│    │    └─Bottleneck (0)                         [20, 256, 64, 64]         [20, 512, 32, 32]         379,392                     1.17%                   --                        9,730,836,480             True\n",
      "│    │    └─Bottleneck (1)                         [20, 512, 32, 32]         [20, 512, 32, 32]         280,064                     0.86%                   --                        5,704,284,160             True\n",
      "│    │    └─Bottleneck (2)                         [20, 512, 32, 32]         [20, 512, 32, 32]         280,064                     0.86%                   --                        5,704,284,160             True\n",
      "│    │    └─Bottleneck (3)                         [20, 512, 32, 32]         [20, 512, 32, 32]         280,064                     0.86%                   --                        5,704,284,160             True\n",
      "│    └─Sequential (layer3)                         [20, 512, 32, 32]         [20, 1024, 16, 16]        --                             --                   --                        --                        True\n",
      "│    │    └─Bottleneck (0)                         [20, 512, 32, 32]         [20, 1024, 16, 16]        1,512,448                   4.65%                   --                        9,730,887,680             True\n",
      "│    │    └─Bottleneck (1)                         [20, 1024, 16, 16]        [20, 1024, 16, 16]        1,117,184                   3.44%                   --                        5,704,314,880             True\n",
      "│    │    └─Bottleneck (2)                         [20, 1024, 16, 16]        [20, 1024, 16, 16]        1,117,184                   3.44%                   --                        5,704,314,880             True\n",
      "│    │    └─Bottleneck (3)                         [20, 1024, 16, 16]        [20, 1024, 16, 16]        1,117,184                   3.44%                   --                        5,704,314,880             True\n",
      "│    │    └─Bottleneck (4)                         [20, 1024, 16, 16]        [20, 1024, 16, 16]        1,117,184                   3.44%                   --                        5,704,314,880             True\n",
      "│    │    └─Bottleneck (5)                         [20, 1024, 16, 16]        [20, 1024, 16, 16]        1,117,184                   3.44%                   --                        5,704,314,880             True\n",
      "│    └─Sequential (layer4)                         [20, 1024, 16, 16]        [20, 2048, 8, 8]          --                             --                   --                        --                        True\n",
      "│    │    └─Bottleneck (0)                         [20, 1024, 16, 16]        [20, 2048, 8, 8]          6,039,552                  18.57%                   --                        9,730,990,080             True\n",
      "│    │    └─Bottleneck (1)                         [20, 2048, 8, 8]          [20, 2048, 8, 8]          4,462,592                  13.72%                   --                        5,704,376,320             True\n",
      "│    │    └─Bottleneck (2)                         [20, 2048, 8, 8]          [20, 2048, 8, 8]          4,462,592                  13.72%                   --                        5,704,376,320             True\n",
      "├─UnetDecoder (decoder)                            [20, 3, 256, 256]         [20, 16, 256, 256]        --                             --                   --                        --                        True\n",
      "│    └─Identity (center)                           [20, 2048, 8, 8]          [20, 2048, 8, 8]          --                             --                   --                        --                        --\n",
      "│    └─ModuleList (blocks)                         --                        --                        --                             --                   --                        --                        True\n",
      "│    │    └─DecoderBlock (0)                       [20, 2048, 8, 8]          [20, 256, 16, 16]         7,668,736                  23.58%                   --                        39,258,705,920            True\n",
      "│    │    └─DecoderBlock (1)                       [20, 256, 16, 16]         [20, 128, 32, 32]         1,032,704                   3.18%                   --                        21,139,302,400            True\n",
      "│    │    └─DecoderBlock (2)                       [20, 128, 32, 32]         [20, 64, 64, 64]          258,304                     0.79%                   --                        21,139,297,280            True\n",
      "│    │    └─DecoderBlock (3)                       [20, 64, 64, 64]          [20, 32, 128, 128]        46,208                      0.14%                   --                        15,099,496,960            True\n",
      "│    │    └─DecoderBlock (4)                       [20, 32, 128, 128]        [20, 16, 256, 256]        6,976                       0.02%                   --                        9,059,697,920             True\n",
      "├─SegmentationHead (segmentation_head)             [20, 16, 256, 256]        [20, 2, 256, 256]         --                             --                   --                        --                        True\n",
      "│    └─Conv2d (0)                                  [20, 16, 256, 256]        [20, 2, 256, 256]         290                         0.00%                   [3, 3]                    380,108,800               True\n",
      "│    └─Identity (1)                                [20, 2, 256, 256]         [20, 2, 256, 256]         --                             --                   --                        --                        --\n",
      "│    └─Activation (2)                              [20, 2, 256, 256]         [20, 2, 256, 256]         --                             --                   --                        --                        --\n",
      "│    │    └─ReLU (activation)                      [20, 2, 256, 256]         [20, 2, 256, 256]         --                             --                   --                        --                        --\n",
      "=================================================================================================================================================================================================================================\n",
      "Total params: 32,521,250\n",
      "Trainable params: 32,521,250\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 212.84\n",
      "=================================================================================================================================================================================================================================\n",
      "Input size (MB): 15.73\n",
      "Forward/backward pass size (MB): 5966.40\n",
      "Params size (MB): 130.09\n",
      "Estimated Total Size (MB): 6112.21\n",
      "=================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sega/progs/AI_Tasks/AIenv/lib/python3.10/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/sega/progs/AI_Tasks/AIenv/lib/python3.10/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    }
   ],
   "source": [
    "# images, _ = next(iter(valid_dataloader))\n",
    "# TBwriter.add_graph(model, images)\n",
    "\n",
    "if \"ONNX\" in SAVE_METHOD:\n",
    "    model_path = f\"{TBpath}/model_first.onnx\"\n",
    "    torch.onnx.export(model, torch.empty(size=(BATCH_SIZE, 3, *CROP_SIZE)), model_path)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(model_sum := torchinfo.summary(model, input_size=(BATCH_SIZE, 3, *CROP_SIZE), row_settings=[\"var_names\"], verbose=0, col_names=[\n",
    "      \"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\", \"trainable\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.losses.DiceLoss(mode='binary')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, threshold=1e-3, cooldown=1, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаги обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(net, criterion, optimizer, dataloader, epoch: int = None):\n",
    "    net.train()\n",
    "    running_loss = 0.\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_loss = running_loss / len(dataloader)\n",
    "    return train_loss.item()\n",
    "\n",
    "\n",
    "def valid_step(net, criterion, dataloader, epoch: int = None):\n",
    "    net.eval()\n",
    "    running_loss = 0.\n",
    "    IoU = metrics.BinaryJaccardIndex()\n",
    "    IoU.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            output = net(images)\n",
    "\n",
    "            IoU(output, labels)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss\n",
    "\n",
    "            save_imgs(pjoin(TBpath, f\"valid_samples/samples_{epoch}\"), name=f\"img_{step}\",\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES))\n",
    "\n",
    "        TBwriter.add_figure('valid_sample', create_image_plot(\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES)),\n",
    "                  epoch)\n",
    "\n",
    "        valid_loss = running_loss / len(valid_dataloader)\n",
    "\n",
    "        return valid_loss.item(), IoU.compute().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = STARTING_EPOCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_WEIGHTS is not None:\n",
    "    model.state_dict(torch.load(LOAD_WEIGHTS))\n",
    "if LOAD_ADAM_STATE is not None:\n",
    "    optimizer.load_state_dict(torch.load(LOAD_ADAM_STATE))\n",
    "    \n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ba1df419ef4cfeb63aa4002adfa034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m pbar\u001b[39m.\u001b[39mupdate(epoch)\n\u001b[1;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m(epoch \u001b[39m<\u001b[39m EPOCHS):\n\u001b[0;32m----> 8\u001b[0m     train_loss \u001b[39m=\u001b[39m train_step(model, loss, optimizer, train_dataloader, epoch)\n\u001b[1;32m      9\u001b[0m     valid_loss, iou_score \u001b[39m=\u001b[39m valid_step(model, loss, valid_dataloader, epoch)\n\u001b[1;32m     10\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(valid_loss)\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(net, criterion, optimizer, dataloader, epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m      6\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 8\u001b[0m optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m     10\u001b[0m output \u001b[39m=\u001b[39m net(images)\n\u001b[1;32m     11\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels)\n",
      "File \u001b[0;32m~/progs/AI_Tasks/AIenv/lib/python3.10/site-packages/torch/optim/optimizer.py:435\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    431\u001b[0m     param_groups \u001b[39m=\u001b[39m [\n\u001b[1;32m    432\u001b[0m         update_group(g, ng) \u001b[39mfor\u001b[39;00m g, ng \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(groups, saved_groups)]\n\u001b[1;32m    433\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__setstate__({\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m: state, \u001b[39m'\u001b[39m\u001b[39mparam_groups\u001b[39m\u001b[39m'\u001b[39m: param_groups})\n\u001b[0;32m--> 435\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mzero_grad\u001b[39m(\u001b[39mself\u001b[39m, set_to_none: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    436\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sets the gradients of all optimized :class:`torch.Tensor` s to zero.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39m            the step altogether).\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    450\u001b[0m     foreach \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mforeach\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = 10000\n",
    "trained = True\n",
    "\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "pbar.update(epoch)\n",
    "\n",
    "while(epoch < EPOCHS):\n",
    "    train_loss = train_step(model, loss, optimizer, train_dataloader, epoch)\n",
    "    valid_loss, iou_score = valid_step(model, loss, valid_dataloader, epoch)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    if WEIGHT_SAVER != \"nothing\" and valid_loss < best_loss and epoch > 3:\n",
    "        best_loss = valid_loss\n",
    "\n",
    "        print(f\"[{epoch}] Saved weights with IoU: {iou_score:.2f} | loss: {valid_loss:.4f}\")\n",
    "    \n",
    "        \n",
    "        if WEIGHT_SAVER == \"all\":\n",
    "            weights_path = f\"{TBpath}/weights_{epoch}.pth\"\n",
    "            model_path = f\"{TBpath}/model_{epoch}.onnx\"\n",
    "            optimizer_path = f\"{TBpath}/optimizer_{epoch}.pth\"\n",
    "            \n",
    "        elif WEIGHT_SAVER == \"last\":\n",
    "            weights_path = f\"{TBpath}/weights_last.pth\"\n",
    "            model_path =   f\"{TBpath}/model_last.onnx\"\n",
    "            optimizer_path = f\"{TBpath}/optimizer_last.pth\"\n",
    "\n",
    "        if \"TORCH\" in SAVE_METHOD:\n",
    "            torch.save(model.state_dict(), weights_path)\n",
    "        \n",
    "        if \"ONNX\" in SAVE_METHOD:\n",
    "            torch.onnx.export(model, torch.empty(size=(BATCH_SIZE, 3, *CROP_SIZE)), model_path)\n",
    "        \n",
    "        torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "\n",
    "    TBwriter.add_scalar('valid loss', valid_loss, epoch)\n",
    "    TBwriter.add_scalar('train loss', train_loss, epoch)\n",
    "    \n",
    "    TBwriter.add_scalar('IoU', iou_score, epoch)\n",
    "\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        TBwriter.add_scalar('learning rate', float(param_group['lr']), epoch)\n",
    "\n",
    "    epoch += 1\n",
    "    pbar.update()\n",
    "    pbar.set_description(\n",
    "        f'IoU: {iou_score:.2f}  | train/valid loss: {train_loss:.4f}/{valid_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется не обученная модель, происходит загрузка модели из /home/sega/progs/AI_Tasks/02_Task/TB_cache/MyUnet_Fixedset_augmented/weights_last.pth\n",
      "Попытка импорта модели из onnx файла\n",
      "Попытка импорта модели из pth файла\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sega/progs/AI_Tasks/AIenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=NORMALIZE_MEAN_IMG, std=NORMALIZE_DEVIATIONS_IMG, always_apply=True),\n",
    "    ]\n",
    ")\n",
    "# как работает сеть на валидации\n",
    "test_dataset = RoadsDataset(*TEST_SET,\n",
    "       class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform, readyToNetwork=prepare_to_network)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=36,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "if not trained:\n",
    "    print(f\"Используется не обученная модель, происходит загрузка модели из {SAVED_MODEL_PATH}\")\n",
    "    model = None\n",
    "    if \"ONNX\" in SAVE_METHOD and model is None:\n",
    "        print(f\"Попытка импорта модели из onnx файла\")\n",
    "        try:\n",
    "            import onnx\n",
    "            model = onnx.load(SAVED_MODEL_PATH)\n",
    "        except:\n",
    "            pass\n",
    "    if \"TORCH\" in SAVE_METHOD and model is None:\n",
    "        print(f\"Попытка импорта модели из pth файла\")\n",
    "        model = MyUnet(2)\n",
    "        #     model = smp.Unet(\n",
    "        #     encoder_name=ENCODER, \n",
    "        #     classes=len(CLASS_NAMES),\n",
    "        #     activation=ACTIVATION,\n",
    "        # )\n",
    "        model.state_dict(torch.load(f=SAVED_MODEL_PATH))\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_METRIC = metrics.BinaryStatScores(average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDivide(x, y): return torch.nan_to_num(x/y)\n",
    "\n",
    "def calculate_metric_by_errors(numerator, denominator, classes):\n",
    "    with torch.no_grad():\n",
    "        metric_values = saveDivide(numerator, denominator)\n",
    "        metric_per_class = {classname: val.item()\n",
    "                            for classname, val in zip(classes, metric_values)}\n",
    "        metric_average = torch.sum(metric_values)/len(classes)\n",
    "        metric_average_micro = saveDivide(torch.sum(numerator), torch.sum(denominator))\n",
    "        return (metric_values, metric_per_class, metric_average, metric_average_micro)\n",
    "\n",
    "def test_step(model, loader, metric : metrics.MulticlassStatScores):\n",
    "    classes = CLASS_NAMES\n",
    "    metric.to(DEVICE)\n",
    "\n",
    "    iou = metrics.BinaryJaccardIndex().to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for id, (images, labels) in enumerate(loader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            output = model(images)\n",
    "            TBwriter.add_figure('test_sample', create_image_plot(\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASS_RGB_VALUES)),\n",
    "                  id)\n",
    "            iou.update(output, labels)\n",
    "            metric.update(output, labels)\n",
    "\n",
    "    tp, fp, tn, fn = metric._final_state()\n",
    "\n",
    "    acc = calculate_metric_by_errors((tp+tn), (tp+fp+tn+fn), classes=classes)\n",
    "    rec = calculate_metric_by_errors(tp, (tp+fn), classes=classes)\n",
    "    prec = calculate_metric_by_errors(tp, (tp+fp), classes=classes)\n",
    "\n",
    "    jaccard = calculate_metric_by_errors(tp, (tp+fp+fn), classes=classes)\n",
    "    dice = calculate_metric_by_errors(2*tp, 2*tp+tn+tp, classes=classes)\n",
    "\n",
    "    metric_values = {\n",
    "        \"accuracy\": acc,\n",
    "        \"recall\": rec,\n",
    "        \"precision\": prec,\n",
    "        \"jaccard\": jaccard,\n",
    "        \"dice\": dice\n",
    "    }\n",
    "\n",
    "    return metric_values, iou.compute().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_values, iou = test_step(model, test_dataloader, TEST_METRIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.0000e-01, 1.7300e-07], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(metric_values[\"jaccard\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:\n",
      "    \tmacro: 0.500\n",
      "    \tmicro: 0.500\n",
      "recall:\n",
      "    \tmacro: 0.500\n",
      "    \tmicro: 0.500\n",
      "precision:\n",
      "    \tmacro: 0.750\n",
      "    \tmicro: 0.500\n",
      "jaccard:\n",
      "    \tmacro: 0.250\n",
      "    \tmicro: 0.333\n",
      "dice:\n",
      "    \tmacro: 0.333\n",
      "    \tmicro: 0.500\n",
      "jaccardIndex:\n",
      "macro: 0.25000008940696716\n"
     ]
    }
   ],
   "source": [
    "metric_count = len(metric_values)\n",
    "\n",
    "fig, axes = plt.subplots(1, metric_count, figsize=(16,8))\n",
    "\n",
    "result_string = \"\"\n",
    "\n",
    "for (metricName, mValues), ax in zip(metric_values.items(), axes):\n",
    "    result_string += f\"\"\"{metricName}:\n",
    "    \\tmacro: {mValues[2]:.3f}\n",
    "    \\tmicro: {mValues[3]:.3f}\\n\"\"\"\n",
    "    mVal = mValues[1]\n",
    "    plt.sca(ax)\n",
    "    plt.bar(mVal.keys(), mVal.values())\n",
    "    plt.title(metricName)\n",
    "    plt.grid(axis='y')\n",
    "    plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "result_string += f\"\"\"jaccardIndex:\n",
    "macro: {iou}\"\"\"\n",
    "\n",
    "print(result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBwriter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
