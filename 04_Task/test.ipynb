{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import MST\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_C = 1\n",
    "out_C = 1\n",
    "stride = 1\n",
    "padding = 1\n",
    "dilation = 1\n",
    "kernel = 3\n",
    "\n",
    "params = [in_C, out_C, kernel, stride, padding, dilation]\n",
    "\n",
    "print(f\"{(16 - kernel + padding*2 - (kernel-1)*(dilation-1))/stride + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    myConv = MST.Conv2d(*params)\n",
    "    torchConv = nn.Conv2d(*params)\n",
    "\n",
    "    torchConv.weight = nn.Parameter(torch.tensor(myConv._w))\n",
    "    torchConv.bias = nn.Parameter(torch.tensor(myConv._bias).flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.ones((1, in_C, 16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    myres = myConv(image)\n",
    "    nnres = torchConv(torch.Tensor(image))\n",
    "    print(myres.shape)\n",
    "    print(nnres.shape)\n",
    "\n",
    "    nnres = nnres.numpy().round(3).flatten()\n",
    "    myres = myres.round(3).flatten()\n",
    "    bads = np.where(abs(nnres - myres) > 0.001)\n",
    "    print(myres[:20])\n",
    "    print(nnres[:20])\n",
    "    print(len(bads[0]))\n",
    "    for pos in bads[0][:10]:\n",
    "        print(f\"[{pos}]: {myres[pos]:.3f} {nnres[pos]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "myres = myConv(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "nnres = torchConv(torch.Tensor(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n"
     ]
    }
   ],
   "source": [
    "def to_np_arr(a):\n",
    "    a_ = MST.MDT_ARRAY(a).astype(np.float32) / 255\n",
    "    return a_.reshape(1, *a_.shape)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import MST\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "np.random.seed(42)\n",
    "in_C = 1\n",
    "out_C = 6\n",
    "stride = 1\n",
    "padding = 0\n",
    "dilation = 1\n",
    "kernel = 5\n",
    "inH, InW = 32, 32\n",
    "outH, outW = inH//stride, InW//stride\n",
    "\n",
    "params = [in_C, out_C, kernel, stride, padding, dilation]\n",
    "\n",
    "print(f\"{(16 - kernel + padding*2 - (kernel-1)*(dilation-1))/stride + 1}\")\n",
    "myConv = MST.Conv2d(*params)\n",
    "torchConv = nn.Conv2d(*params)\n",
    "\n",
    "torchConv.weight = nn.Parameter(torch.tensor(myConv._w))\n",
    "torchConv.bias = nn.Parameter(torch.tensor(myConv._bias).flatten())\n",
    "image = np.ones((1, in_C, inH, InW))\n",
    "\n",
    "transform = to_np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a2b1a861104858a733e779610338da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m myres\u001b[39m.\u001b[39mbackward(dOut)\n\u001b[1;32m     30\u001b[0m nnres \u001b[39m=\u001b[39m torchConv(torchImage)\n\u001b[0;32m---> 31\u001b[0m nnres\u001b[39m.\u001b[39;49mbackward(torch\u001b[39m.\u001b[39;49mTensor(dOut))\n\u001b[1;32m     33\u001b[0m \u001b[39m# nngrad = torchImage.grad.numpy().round(3).flatten()\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# mygrad = myConv._dinX.round(3).flatten()\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# bads = np.where(abs(mygrad - nngrad) > 0.001)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# print(len(bads[0]))\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m# for pos in bads[0][:10]:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m#     print(f\"[{pos}]: {mygrad[pos]:.3f} {nngrad[pos]:.3f}\")\u001b[39;00m\n\u001b[1;32m     40\u001b[0m nnres \u001b[39m=\u001b[39m nnres\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mround(\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/progs/AI_Tasks/AIenv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/progs/AI_Tasks/AIenv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# torchImage = torch.ones((1, in_C, inH, InW), requires_grad=True)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='datasets',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "for (images, labels) in tqdm(train_dataloader):\n",
    "\n",
    "    torchImage = images\n",
    "    image = torchImage.detach().numpy()\n",
    "    \n",
    "    out_sample = myConv(image)\n",
    "\n",
    "    dOut = np.random.random(out_sample.shape)\n",
    "\n",
    "    myres = myConv(image)\n",
    "    myres.backward(dOut)\n",
    "    nnres = torchConv(torchImage)\n",
    "    nnres.backward(torch.Tensor(dOut))\n",
    "\n",
    "    # nngrad = torchImage.grad.numpy().round(3).flatten()\n",
    "    # mygrad = myConv._dinX.round(3).flatten()\n",
    "    # bads = np.where(abs(mygrad - nngrad) > 0.001)\n",
    "    # print(len(bads[0]))\n",
    "    # for pos in bads[0][:10]:\n",
    "    #     print(f\"[{pos}]: {mygrad[pos]:.3f} {nngrad[pos]:.3f}\")\n",
    "\n",
    "    nnres = nnres.detach().numpy().round(3).flatten()\n",
    "    myres = myres.round(3).flatten()\n",
    "    bads = np.where(abs(nnres - myres) > 0.001)\n",
    "    print(len(bads[0]))\n",
    "    for pos in bads[0][:10]:\n",
    "        print(f\"[{pos}]: {myres[pos]:.3f} {nnres[pos]:.3f}\")\n",
    "\n",
    "    nngrad = torchConv.weight.grad.numpy().round(3).flatten()\n",
    "    mygrad = myConv._dw.round(3).flatten()\n",
    "    bads = np.where(abs(mygrad - nngrad) > 0.001)\n",
    "    print(len(bads[0]))\n",
    "    for pos in bads[0][:10]:\n",
    "        print(f\"[{pos}]: {mygrad[pos]:.3f} {nngrad[pos]:.3f}\")\n",
    "\n",
    "    torchConv.weight.grad = torch.zeros_like(torchConv.weight.grad)\n",
    "\n",
    "    nngrad = torchConv.bias.grad.numpy().round(3).flatten()\n",
    "    mygrad = myConv._dbias.round(3).flatten()\n",
    "    bads = np.where(abs(mygrad - nngrad) > 0.001)\n",
    "    print(len(bads[0]))\n",
    "    for pos in bads[0][:10]:\n",
    "        print(f\"[{pos}]: {mygrad[pos]:.3f} {nngrad[pos]:.3f}\")\n",
    "\n",
    "    torchConv.bias.grad = torch.zeros_like(torchConv.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 30 is out of bounds for axis 2 with size 30",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     myres \u001b[39m=\u001b[39m myConv(image)\n\u001b[1;32m      3\u001b[0m     nnres \u001b[39m=\u001b[39m torchConv(torch\u001b[39m.\u001b[39mTensor(image))\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(myres\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/progs/AI_Tasks/04_Task/MST/BasicModules.py:74\u001b[0m, in \u001b[0;36mBasicModule.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hid_inX\u001b[39m.\u001b[39mappend(arg)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hid_outX \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hid_outX, MDT_REFACTOR_ARRAY) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hid_outX, ndarray)):\n\u001b[1;32m     76\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hid_outX \u001b[39m=\u001b[39m MDT_ARRAY(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hid_outX)\n",
      "File \u001b[0;32m~/progs/AI_Tasks/04_Task/MST/Layers/Convolution.py:30\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate_output_sizes(H, W)\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inX \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpad(x, [(\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m), (\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m), (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_padding[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_padding[\u001b[39m0\u001b[39m]), (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_padding[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_padding[\u001b[39m1\u001b[39m])], mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m'\u001b[39m, constant_values\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_padding_value)\n\u001b[0;32m---> 30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inX_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_im2col(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inX) \u001b[39m# .shape() = [C * K * K, BS * H * W]\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flatten_w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_w\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outC, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# .shape() = [outC, inC * K * K]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outX \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flatten_w, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inX_cols)\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outC, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_size, BS)\n",
      "File \u001b[0;32m~/progs/AI_Tasks/04_Task/MST/Layers/Basic/BasicLayers.py:66\u001b[0m, in \u001b[0;36mConvBasicModule._im2col\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__im2col_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__im2col_indices_calc(image\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     64\u001b[0m i, j, k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__im2col_indices\n\u001b[0;32m---> 66\u001b[0m cols \u001b[39m=\u001b[39m image[:, k, i, j]\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mreshape(kernel_h\u001b[39m*\u001b[39mkernel_w\u001b[39m*\u001b[39mC, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m cols\n",
      "\u001b[0;31mIndexError\u001b[0m: index 30 is out of bounds for axis 2 with size 30"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    myres = myConv(image)\n",
    "    nnres = torchConv(torch.Tensor(image))\n",
    "    print(myres.shape)\n",
    "    print(nnres.shape)\n",
    "\n",
    "    nnres = nnres.numpy().round(3).flatten()\n",
    "    myres = myres.round(3).flatten()\n",
    "    bads = np.where(abs(nnres - myres) > 0.001)\n",
    "    print(myres[:20])\n",
    "    print(nnres[:20])\n",
    "    print(len(bads[0]))\n",
    "    for pos in bads[0][:10]:\n",
    "        print(f\"[{pos}]: {myres[pos]:.3f} {nnres[pos]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import MST\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "BS = 513\n",
    "in_C = 1\n",
    "out_C = 6\n",
    "stride = 4\n",
    "padding = 2\n",
    "dilation = 1\n",
    "kernel = 5\n",
    "inH, InW = 256, 256\n",
    "outH, outW = inH//stride, InW//stride\n",
    "\n",
    "params = [kernel, stride, padding, dilation]\n",
    "\n",
    "# print(f\"{(16 - kernel + padding*2 - (kernel-1)*(dilation-1))/stride + 1}\")\n",
    "myConv = MST.MaxPool2d(*params)\n",
    "torchConv = nn.MaxPool2d(*params)\n",
    "\n",
    "# torchConv.weight = nn.Parameter(torch.tensor(myConv._w))\n",
    "# torchConv.bias = nn.Parameter(torch.tensor(myConv._bias).flatten())\n",
    "# image = np.ones((BS, in_C, inH, InW))\n",
    "torchImage = torch.rand((BS, in_C, inH, InW), requires_grad=True)\n",
    "image = torchImage.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 1, 64, 64)\n",
      "torch.Size([513, 1, 64, 64])\n",
      "[0.935 0.945 0.986 0.976 0.89  0.944 0.909 0.951 0.964 0.95  0.913 0.989\n",
      " 0.927 0.968 0.992 0.944]\n",
      "[0.935 0.945 0.986 0.976 0.89  0.944 0.909 0.951 0.964 0.95  0.913 0.989\n",
      " 0.927 0.968 0.992 0.944]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    myres = myConv(image)\n",
    "    nnres = torchConv(torch.Tensor(image))\n",
    "    print(myres.shape)\n",
    "    print(nnres.shape)\n",
    "\n",
    "    nnres = nnres.numpy().round(3).flatten()\n",
    "    myres = myres.round(3).flatten()\n",
    "    bads = np.where(abs(nnres - myres) > 0.001)\n",
    "    print(myres[54:70])\n",
    "    print(nnres[54:70])\n",
    "    print(len(bads[0]))\n",
    "    for pos in bads[0][:10]:\n",
    "        print(f\"[{pos}]: {myres[pos]:.3f} {nnres[pos]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MaxPool2d' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m pos \u001b[39min\u001b[39;00m bads[\u001b[39m0\u001b[39m][:\u001b[39m10\u001b[39m]:\n\u001b[1;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mpos\u001b[39m}\u001b[39;00m\u001b[39m]: \u001b[39m\u001b[39m{\u001b[39;00mmygrad[pos]\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mnngrad[pos]\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m nngrad \u001b[39m=\u001b[39m torchConv\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mround(\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     16\u001b[0m mygrad \u001b[39m=\u001b[39m myConv\u001b[39m.\u001b[39m_dw\u001b[39m.\u001b[39mround(\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     17\u001b[0m bads \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(\u001b[39mabs\u001b[39m(mygrad \u001b[39m-\u001b[39m nngrad) \u001b[39m>\u001b[39m \u001b[39m0.001\u001b[39m)\n",
      "File \u001b[0;32m~/progs/AI_Tasks/AIenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaxPool2d' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "out_sample = myConv(image)\n",
    "\n",
    "myConv(image).backward(np.ones(out_sample.shape))\n",
    "torchConv(torchImage).backward(torch.ones(out_sample.shape))\n",
    "\n",
    "\n",
    "nngrad = torchImage.grad.numpy().round(3).flatten()\n",
    "mygrad = myConv._dinX.round(3).flatten()\n",
    "bads = np.where(abs(mygrad - nngrad) > 0.001)\n",
    "print(len(bads[0]))\n",
    "for pos in bads[0][:10]:\n",
    "    print(f\"[{pos}]: {mygrad[pos]:.3f} {nngrad[pos]:.3f}\")\n",
    "\n",
    "\n",
    "nngrad = torchConv.weight.grad.numpy().round(3).flatten()\n",
    "mygrad = myConv._dw.round(3).flatten()\n",
    "bads = np.where(abs(mygrad - nngrad) > 0.001)\n",
    "print(len(bads[0]))\n",
    "for pos in bads[0][:10]:\n",
    "    print(f\"[{pos}]: {mygrad[pos]:.3f} {nngrad[pos]:.3f}\")\n",
    "\n",
    "torchConv.weight.grad = torch.zeros_like(torchConv.weight.grad)\n",
    "\n",
    "nngrad = torchConv.bias.grad.numpy().round(3).flatten()\n",
    "mygrad = myConv._dbias.round(3).flatten()\n",
    "bads = np.where(abs(mygrad - nngrad) > 0.001)\n",
    "print(len(bads[0]))\n",
    "for pos in bads[0][:10]:\n",
    "    print(f\"[{pos}]: {mygrad[pos]:.3f} {nngrad[pos]:.3f}\")\n",
    "\n",
    "torchConv.bias.grad = torch.zeros_like(torchConv.bias.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
